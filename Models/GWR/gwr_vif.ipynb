{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading new datasets...\n",
      "Defining target variable and features...\n",
      "Converting features and target variable to numeric...\n",
      "Checking for zero-variance columns...\n",
      "Initial Condition number of X_train: 1.54e+05\n",
      "Checking for multicollinearity using VIF...\n",
      "Dropping year due to high VIF (54.34)\n",
      "Dropping PreSchool_within_1km due to high VIF (20.52)\n",
      "Dropping lease_commence_date due to high VIF (12.22)\n",
      "Dropping flat_type_LE due to high VIF (9.71)\n",
      "Dropping Primary_within_1km due to high VIF (8.67)\n",
      "Dropping price_per_sqm due to high VIF (6.86)\n",
      "Dropping flat_model_LE due to high VIF (5.68)\n",
      "Applying jitter to geographical coordinates...\n",
      "Scaling features for numerical stability...\n",
      "Condition number after scaling: 2.01e+00\n",
      "Extracting geographical coordinates...\n",
      "Selecting optimal bandwidth using cross-validation...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 106\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m     selector \u001b[38;5;241m=\u001b[39m Sel_BW(coords_train, train_data[target_variable]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), X_train_scaled)\n\u001b[1;32m--> 106\u001b[0m     optimal_bandwidth \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimal Bandwidth: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimal_bandwidth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mLinAlgError:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mgwr\\sel_bw.py:324\u001b[0m, in \u001b[0;36mSel_BW.search\u001b[1;34m(self, search_method, criterion, bw_min, bw_max, interval, tol, max_iter, init_multi, tol_multi, rss_score, max_iter_multi, multi_bw_min, multi_bw_max, bws_same_times, verbose, pool)\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbw_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbw[\n\u001b[0;32m    322\u001b[0m         \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m#scalar, optimal bw from initial gwr model\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 324\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msel_hist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbw[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbw[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mgwr\\sel_bw.py:341\u001b[0m, in \u001b[0;36mSel_BW._bw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    338\u001b[0m     a, c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_section(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_glob, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_loc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoords,\n\u001b[0;32m    339\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstant)\n\u001b[0;32m    340\u001b[0m     delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.38197\u001b[39m  \u001b[38;5;66;03m#1 - (np.sqrt(5.0)-1.0)/2.0\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbw \u001b[38;5;241m=\u001b[39m \u001b[43mgolden_section\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgwr_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbw_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterval\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbw \u001b[38;5;241m=\u001b[39m equal_interval(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbw_min, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbw_max, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterval,\n\u001b[0;32m    346\u001b[0m                              gwr_func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mint_score, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mgwr\\search.py:66\u001b[0m, in \u001b[0;36mgolden_section\u001b[1;34m(a, c, delta, function, tol, max_iter, bw_max, int_score, verbose)\u001b[0m\n\u001b[0;32m     64\u001b[0m     score_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m[b]\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 66\u001b[0m     score_b \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28mdict\u001b[39m[b] \u001b[38;5;241m=\u001b[39m score_b\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mgwr\\sel_bw.py:333\u001b[0m, in \u001b[0;36mSel_BW._bw.<locals>.<lambda>\u001b[1;34m(bw)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_bw\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    330\u001b[0m     gwr_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m bw: getDiag[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion](\u001b[43mGWR\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_loc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\n\u001b[0;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\n\u001b[1;32m--> 333\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspherical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspherical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimized_function \u001b[38;5;241m=\u001b[39m gwr_func\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgolden_section\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mgwr\\gwr.py:348\u001b[0m, in \u001b[0;36mGWR.fit\u001b[1;34m(self, ini_params, tol, max_iter, solve, lite, pool)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    346\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoints\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 348\u001b[0m rslt \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_local_fit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    350\u001b[0m rslt_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mrslt))\n\u001b[0;32m    351\u001b[0m influ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(rslt_list[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import cond, matrix_rank\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from mgwr.gwr import GWR\n",
    "from mgwr.sel_bw import Sel_BW\n",
    "\n",
    "# Load new datasets\n",
    "print(\"Loading new datasets...\")\n",
    "train_data = pd.read_csv(\"new_normalized_train.csv\").sample(50000, random_state=42)\n",
    "test_data = pd.read_csv(\"new_normalized_test.csv\").sample(50000, random_state=42)\n",
    "\n",
    "# Define target variable and features\n",
    "print(\"Defining target variable and features...\")\n",
    "target_variable = \"resale_price\"\n",
    "model_columns = [\n",
    "    \"month\", \"year\", \"town_LE\", \"flat_type_LE\", \"storey_range_LE\",  \n",
    "    \"price_per_sqm\", \"flat_model_LE\", \"lease_commence_date\", \"Latitude\", \"Longitude\", \n",
    "    \"LTAMRTStation_within_1km\", \"MallCoordinates_within_1km\", \"Hawker_within_1km\", \n",
    "    \"PreSchool_within_1km\", \"Primary_within_1km\", \"Secondary_within_1km\", \n",
    "    \"JuniorCollege_within_1km\", \"MixedLevel_within_1km\", \"NParks_within_1km\", \"Sports_within_1km\"\n",
    "]\n",
    "\n",
    "# Convert features and target variable to numeric\n",
    "print(\"Converting features and target variable to numeric...\")\n",
    "train_data[model_columns] = train_data[model_columns].apply(pd.to_numeric, errors='coerce')\n",
    "test_data[model_columns] = test_data[model_columns].apply(pd.to_numeric, errors='coerce')\n",
    "train_data[target_variable] = pd.to_numeric(train_data[target_variable], errors='coerce')\n",
    "test_data[target_variable] = pd.to_numeric(test_data[target_variable], errors='coerce')\n",
    "\n",
    "# Remove zero-variance columns\n",
    "print(\"Checking for zero-variance columns...\")\n",
    "zero_var_cols = [col for col in model_columns if train_data[col].nunique() == 1]\n",
    "if zero_var_cols:\n",
    "    print(f\"Dropping zero-variance columns: {zero_var_cols}\")\n",
    "    train_data.drop(columns=zero_var_cols, inplace=True)\n",
    "    test_data.drop(columns=zero_var_cols, inplace=True)\n",
    "    model_columns = [col for col in model_columns if col not in zero_var_cols]\n",
    "\n",
    "# Check condition number before processing\n",
    "cond_number = np.linalg.cond(train_data[model_columns].values)\n",
    "print(f\"Initial Condition number of X_train: {cond_number:.2e}\")\n",
    "\n",
    "# Remove highly collinear features using VIF (excluding Longitude and Latitude)\n",
    "print(\"Checking for multicollinearity using VIF...\")\n",
    "def calculate_vif(df):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = df.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "vif_columns = [col for col in model_columns if col not in [\"Longitude\", \"Latitude\"]]  # Keep these in the model\n",
    "while True:\n",
    "    vif_df = calculate_vif(train_data[vif_columns])\n",
    "    max_vif = vif_df[\"VIF\"].max()\n",
    "    if max_vif > 4:  # Lower VIF threshold to remove high collinearity\n",
    "        feature_to_drop = vif_df.loc[vif_df[\"VIF\"].idxmax(), \"Feature\"]\n",
    "        print(f\"Dropping {feature_to_drop} due to high VIF ({max_vif:.2f})\")\n",
    "        train_data.drop(columns=[feature_to_drop], inplace=True)\n",
    "        test_data.drop(columns=[feature_to_drop], inplace=True)\n",
    "        vif_columns.remove(feature_to_drop)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Check for low-variance features again after VIF filtering\n",
    "low_var_cols = [col for col in vif_columns if train_data[col].std() < 1e-4]\n",
    "if low_var_cols:\n",
    "    print(f\"Dropping low-variance columns: {low_var_cols}\")\n",
    "    train_data.drop(columns=low_var_cols, inplace=True)\n",
    "    test_data.drop(columns=low_var_cols, inplace=True)\n",
    "    vif_columns = [col for col in vif_columns if col not in low_var_cols]\n",
    "\n",
    "# Slightly jitter geographical coordinates to ensure uniqueness\n",
    "print(\"Applying jitter to geographical coordinates...\")\n",
    "train_data[[\"Longitude\", \"Latitude\"]] += np.random.normal(0, 0.0001, train_data[[\"Longitude\", \"Latitude\"]].shape)\n",
    "test_data[[\"Longitude\", \"Latitude\"]] += np.random.normal(0, 0.0001, test_data[[\"Longitude\", \"Latitude\"]].shape)\n",
    "\n",
    "# Ensure minimum spatial uniqueness threshold\n",
    "unique_locations = len(train_data[[\"Longitude\", \"Latitude\"]].drop_duplicates())\n",
    "if unique_locations / len(train_data) < 0.95:\n",
    "    print(\"❌ ERROR: Too many duplicate spatial points. GWR may fail.\")\n",
    "    exit()\n",
    "\n",
    "# Scale features for numerical stability\n",
    "print(\"Scaling features for numerical stability...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_data[vif_columns])\n",
    "X_test_scaled = scaler.transform(test_data[vif_columns])\n",
    "print(f\"Condition number after scaling: {cond(X_train_scaled):.2e}\")\n",
    "\n",
    "# Extract geographical coordinates\n",
    "print(\"Extracting geographical coordinates...\")\n",
    "coords_train = train_data[['Longitude', 'Latitude']].values\n",
    "coords_test = test_data[['Longitude', 'Latitude']].values\n",
    "\n",
    "# Check matrix rank before proceeding\n",
    "if matrix_rank(X_train_scaled) < X_train_scaled.shape[1]:\n",
    "    print(\"❌ ERROR: Feature matrix is still singular. Skipping GWR model.\")\n",
    "    exit()\n",
    "\n",
    "# Select optimal bandwidth using cross-validation\n",
    "print(\"Selecting optimal bandwidth using cross-validation...\")\n",
    "try:\n",
    "    selector = Sel_BW(coords_train, train_data[target_variable].values.reshape(-1, 1), X_train_scaled)\n",
    "    optimal_bandwidth = selector.search()\n",
    "    print(f\"Optimal Bandwidth: {optimal_bandwidth}\")\n",
    "except np.linalg.LinAlgError:\n",
    "    print(\"❌ ERROR: Matrix is still singular after preprocessing.\")\n",
    "    print(\"Possible cause: Check feature correlation or spatial diversity.\")\n",
    "    optimal_bandwidth = None\n",
    "\n",
    "# Fit the GWR model only if bandwidth selection was successful\n",
    "if optimal_bandwidth is not None:\n",
    "    print(\"Fitting the GWR model...\")\n",
    "    gwr_model = GWR(coords_train, train_data[target_variable].values.reshape(-1, 1), X_train_scaled, bw=optimal_bandwidth)\n",
    "    gwr_results = gwr_model.fit()\n",
    "    print(\"GWR Model Fitted Successfully!\")\n",
    "else:\n",
    "    print(\"Skipping GWR model fitting due to singular matrix issue.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GWR VIF\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import cond, matrix_rank\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from mgwr.gwr import GWR\n",
    "from mgwr.sel_bw import Sel_BW\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load new datasets\n",
    "print(\"Loading new datasets...\")\n",
    "train_data = pd.read_csv(\"new_normalized_train.csv\")\n",
    "test_data = pd.read_csv(\"new_normalized_test.csv\")\n",
    "\n",
    "# Define target variable and features\n",
    "print(\"Defining target variable and features...\")\n",
    "target_variable = \"resale_price\"\n",
    "model_columns = [\n",
    "    \"month\", \"year\", \"town_LE\", \"flat_type_LE\", \"storey_range_LE\",  \n",
    "    \"price_per_sqm\", \"flat_model_LE\", \"lease_commence_date\", \"Latitude\", \"Longitude\", \n",
    "    \"LTAMRTStation_within_1km\", \"MallCoordinates_within_1km\", \"Hawker_within_1km\", \n",
    "    \"PreSchool_within_1km\", \"Primary_within_1km\", \"Secondary_within_1km\", \n",
    "    \"JuniorCollege_within_1km\", \"MixedLevel_within_1km\", \"NParks_within_1km\", \"Sports_within_1km\"\n",
    "]\n",
    "\n",
    "# Convert features and target variable to numeric\n",
    "print(\"Converting features and target variable to numeric...\")\n",
    "train_data[model_columns] = train_data[model_columns].apply(pd.to_numeric, errors='coerce')\n",
    "test_data[model_columns] = test_data[model_columns].apply(pd.to_numeric, errors='coerce')\n",
    "train_data[target_variable] = pd.to_numeric(train_data[target_variable], errors='coerce')\n",
    "test_data[target_variable] = pd.to_numeric(test_data[target_variable], errors='coerce')\n",
    "\n",
    "# Remove zero-variance columns\n",
    "print(\"Checking for zero-variance columns...\")\n",
    "zero_var_cols = [col for col in model_columns if train_data[col].nunique() == 1]\n",
    "if zero_var_cols:\n",
    "    print(f\"Dropping zero-variance columns: {zero_var_cols}\")\n",
    "    train_data.drop(columns=zero_var_cols, inplace=True)\n",
    "    test_data.drop(columns=zero_var_cols, inplace=True)\n",
    "    model_columns = [col for col in model_columns if col not in zero_var_cols]\n",
    "\n",
    "# Check condition number before processing\n",
    "cond_number = np.linalg.cond(train_data[model_columns].values)\n",
    "print(f\"Initial Condition number of X_train: {cond_number:.2e}\")\n",
    "\n",
    "# Remove highly collinear features using VIF (excluding Longitude and Latitude)\n",
    "print(\"Checking for multicollinearity using VIF...\")\n",
    "def calculate_vif(df):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = df.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "vif_columns = [col for col in model_columns if col not in [\"Longitude\", \"Latitude\"]]  # Keep these in the model\n",
    "while True:\n",
    "    vif_df = calculate_vif(train_data[vif_columns])\n",
    "    max_vif = vif_df[\"VIF\"].max()\n",
    "    if max_vif > 4:  # Lower VIF threshold to remove high collinearity\n",
    "        feature_to_drop = vif_df.loc[vif_df[\"VIF\"].idxmax(), \"Feature\"]\n",
    "        print(f\"Dropping {feature_to_drop} due to high VIF ({max_vif:.2f})\")\n",
    "        train_data.drop(columns=[feature_to_drop], inplace=True)\n",
    "        test_data.drop(columns=[feature_to_drop], inplace=True)\n",
    "        vif_columns.remove(feature_to_drop)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Check for low-variance features again after VIF filtering\n",
    "low_var_cols = [col for col in vif_columns if train_data[col].std() < 1e-4]\n",
    "if low_var_cols:\n",
    "    print(f\"Dropping low-variance columns: {low_var_cols}\")\n",
    "    train_data.drop(columns=low_var_cols, inplace=True)\n",
    "    test_data.drop(columns=low_var_cols, inplace=True)\n",
    "    vif_columns = [col for col in vif_columns if col not in low_var_cols]\n",
    "\n",
    "# Slightly jitter geographical coordinates to ensure uniqueness\n",
    "print(\"Applying jitter to geographical coordinates...\")\n",
    "train_data[[\"Longitude\", \"Latitude\"]] += np.random.normal(0, 0.0001, train_data[[\"Longitude\", \"Latitude\"]].shape)\n",
    "test_data[[\"Longitude\", \"Latitude\"]] += np.random.normal(0, 0.0001, test_data[[\"Longitude\", \"Latitude\"]].shape)\n",
    "\n",
    "# Ensure minimum spatial uniqueness threshold\n",
    "unique_locations = len(train_data[[\"Longitude\", \"Latitude\"]].drop_duplicates())\n",
    "if unique_locations / len(train_data) < 0.95:\n",
    "    print(\"❌ ERROR: Too many duplicate spatial points. GWR may fail.\")\n",
    "    exit()\n",
    "\n",
    "# Scale features for numerical stability\n",
    "print(\"Scaling features for numerical stability...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_data[vif_columns])\n",
    "X_test_scaled = scaler.transform(test_data[vif_columns])\n",
    "print(f\"Condition number after scaling: {cond(X_train_scaled):.2e}\")\n",
    "\n",
    "# Extract geographical coordinates\n",
    "print(\"Extracting geographical coordinates...\")\n",
    "coords_train = train_data[['Longitude', 'Latitude']].values\n",
    "coords_test = test_data[['Longitude', 'Latitude']].values\n",
    "\n",
    "# Check matrix rank before proceeding\n",
    "if matrix_rank(X_train_scaled) < X_train_scaled.shape[1]:\n",
    "    print(\"❌ ERROR: Feature matrix is still singular. Skipping GWR model.\")\n",
    "    exit()\n",
    "\n",
    "# Select optimal bandwidth using cross-validation\n",
    "print(\"Selecting optimal bandwidth using cross-validation...\")\n",
    "try:\n",
    "    selector = Sel_BW(coords_train, train_data[target_variable].values.reshape(-1, 1), X_train_scaled)\n",
    "    optimal_bandwidth = selector.search()\n",
    "    print(f\"Optimal Bandwidth: {optimal_bandwidth}\")\n",
    "except np.linalg.LinAlgError:\n",
    "    print(\"❌ ERROR: Matrix is still singular after preprocessing.\")\n",
    "    print(\"Possible cause: Check feature correlation or spatial diversity.\")\n",
    "    optimal_bandwidth = None\n",
    "\n",
    "# Fit the GWR model only if bandwidth selection was successful\n",
    "if optimal_bandwidth is not None:\n",
    "    print(\"Fitting the GWR model...\")\n",
    "    gwr_model = GWR(coords_train, train_data[target_variable].values.reshape(-1, 1), X_train_scaled, bw=optimal_bandwidth)\n",
    "    gwr_results = gwr_model.fit()\n",
    "    print(\"GWR Model Fitted Successfully!\")\n",
    "    gwr_results.summary()\n",
    "    \n",
    "    # Evaluate model performance\n",
    "    print(\"Evaluating GWR model...\")\n",
    "\n",
    "    print('Mean R2 =', gwr_results.R2)\n",
    "    print('AIC =', gwr_results.aic)\n",
    "    print('AICc =', gwr_results.aicc)\n",
    "\n",
    "    # Generate predictions correctly\n",
    "    scale = gwr_results.scale\n",
    "    residuals = gwr_results.resid_response\n",
    "\n",
    "    y_test_pred = gwr_model.predict(coords_test, X_test_scaled, scale, residuals)\n",
    "\n",
    "    # print(f\"y_test_pred: {y_test_pred.predictions}\")\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    mae = mean_absolute_error(test_data[target_variable], y_test_pred.predictions.flatten())\n",
    "    mse = mean_squared_error(test_data[target_variable], y_test_pred.predictions.flatten())\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(test_data[target_variable], y_test_pred.predictions.flatten())\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Skipping GWR model fitting due to singular matrix issue.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading new datasets...\n",
      "Defining target variable and features...\n",
      "Converting features and target variable to numeric...\n",
      "Checking for zero-variance columns...\n",
      "Initial Condition number of X_train: 1.61e+05\n",
      "Checking for multicollinearity using VIF...\n",
      "Dropping year due to high VIF (54.77)\n",
      "Dropping PreSchool_within_1km due to high VIF (20.51)\n",
      "Dropping lease_commence_date due to high VIF (12.22)\n",
      "Dropping flat_type_LE due to high VIF (9.83)\n",
      "Dropping Primary_within_1km due to high VIF (8.73)\n",
      "Dropping price_per_sqm due to high VIF (6.88)\n",
      "Dropping flat_model_LE due to high VIF (5.68)\n",
      "Applying jitter to geographical coordinates...\n",
      "Scaling features for numerical stability...\n",
      "Condition number after scaling: 2.02e+00\n",
      "Extracting geographical coordinates...\n",
      "Selecting optimal bandwidth using cross-validation...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import cond, matrix_rank\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from mgwr.gwr import GWR\n",
    "from mgwr.sel_bw import Sel_BW\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from itertools import product\n",
    "\n",
    "# Load new datasets\n",
    "print(\"Loading new datasets...\")\n",
    "train_data = pd.read_csv(\"new_normalized_train.csv\")\n",
    "test_data = pd.read_csv(\"new_normalized_test.csv\")\n",
    "\n",
    "# Define target variable and features\n",
    "print(\"Defining target variable and features...\")\n",
    "target_variable = \"resale_price\"\n",
    "model_columns = [\n",
    "    \"month\", \"year\", \"town_LE\", \"flat_type_LE\", \"storey_range_LE\",  \n",
    "    \"price_per_sqm\", \"flat_model_LE\", \"lease_commence_date\", \"Latitude\", \"Longitude\", \n",
    "    \"LTAMRTStation_within_1km\", \"MallCoordinates_within_1km\", \"Hawker_within_1km\", \n",
    "    \"PreSchool_within_1km\", \"Primary_within_1km\", \"Secondary_within_1km\", \n",
    "    \"JuniorCollege_within_1km\", \"MixedLevel_within_1km\", \"NParks_within_1km\", \"Sports_within_1km\"\n",
    "]\n",
    "\n",
    "# Convert features and target variable to numeric\n",
    "print(\"Converting features and target variable to numeric...\")\n",
    "train_data[model_columns] = train_data[model_columns].apply(pd.to_numeric, errors='coerce')\n",
    "test_data[model_columns] = test_data[model_columns].apply(pd.to_numeric, errors='coerce')\n",
    "train_data[target_variable] = pd.to_numeric(train_data[target_variable], errors='coerce')\n",
    "test_data[target_variable] = pd.to_numeric(test_data[target_variable], errors='coerce')\n",
    "\n",
    "# Remove zero-variance columns\n",
    "print(\"Checking for zero-variance columns...\")\n",
    "zero_var_cols = [col for col in model_columns if train_data[col].nunique() == 1]\n",
    "if zero_var_cols:\n",
    "    print(f\"Dropping zero-variance columns: {zero_var_cols}\")\n",
    "    train_data.drop(columns=zero_var_cols, inplace=True)\n",
    "    test_data.drop(columns=zero_var_cols, inplace=True)\n",
    "    model_columns = [col for col in model_columns if col not in zero_var_cols]\n",
    "\n",
    "# Check condition number before processing\n",
    "cond_number = np.linalg.cond(train_data[model_columns].values)\n",
    "print(f\"Initial Condition number of X_train: {cond_number:.2e}\")\n",
    "\n",
    "# Remove highly collinear features using VIF (excluding Longitude and Latitude)\n",
    "print(\"Checking for multicollinearity using VIF...\")\n",
    "def calculate_vif(df):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = df.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "vif_columns = [col for col in model_columns if col not in [\"Longitude\", \"Latitude\"]]  # Keep these in the model\n",
    "while True:\n",
    "    vif_df = calculate_vif(train_data[vif_columns])\n",
    "    max_vif = vif_df[\"VIF\"].max()\n",
    "    if max_vif > 4:  # Lower VIF threshold to remove high collinearity\n",
    "        feature_to_drop = vif_df.loc[vif_df[\"VIF\"].idxmax(), \"Feature\"]\n",
    "        print(f\"Dropping {feature_to_drop} due to high VIF ({max_vif:.2f})\")\n",
    "        train_data.drop(columns=[feature_to_drop], inplace=True)\n",
    "        test_data.drop(columns=[feature_to_drop], inplace=True)\n",
    "        vif_columns.remove(feature_to_drop)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Check for low-variance features again after VIF filtering\n",
    "low_var_cols = [col for col in vif_columns if train_data[col].std() < 1e-4]\n",
    "if low_var_cols:\n",
    "    print(f\"Dropping low-variance columns: {low_var_cols}\")\n",
    "    train_data.drop(columns=low_var_cols, inplace=True)\n",
    "    test_data.drop(columns=low_var_cols, inplace=True)\n",
    "    vif_columns = [col for col in vif_columns if col not in low_var_cols]\n",
    "\n",
    "# Slightly jitter geographical coordinates to ensure uniqueness\n",
    "print(\"Applying jitter to geographical coordinates...\")\n",
    "train_data[[\"Longitude\", \"Latitude\"]] += np.random.normal(0, 0.0001, train_data[[\"Longitude\", \"Latitude\"]].shape)\n",
    "test_data[[\"Longitude\", \"Latitude\"]] += np.random.normal(0, 0.0001, test_data[[\"Longitude\", \"Latitude\"]].shape)\n",
    "\n",
    "# Scale features for numerical stability\n",
    "print(\"Scaling features for numerical stability...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_data[vif_columns])\n",
    "X_test_scaled = scaler.transform(test_data[vif_columns])\n",
    "print(f\"Condition number after scaling: {cond(X_train_scaled):.2e}\")\n",
    "\n",
    "# Extract geographical coordinates\n",
    "print(\"Extracting geographical coordinates...\")\n",
    "coords_train = train_data[['Longitude', 'Latitude']].values\n",
    "coords_test = test_data[['Longitude', 'Latitude']].values\n",
    "\n",
    "# Select optimal bandwidth using cross-validation\n",
    "print(\"Selecting optimal bandwidth using cross-validation...\")\n",
    "try:\n",
    "    selector = Sel_BW(coords_train, train_data[target_variable].values.reshape(-1, 1), X_train_scaled)\n",
    "    optimal_bandwidth = selector.search()\n",
    "    print(f\"Optimal Bandwidth: {optimal_bandwidth}\")\n",
    "except np.linalg.LinAlgError:\n",
    "    print(\"❌ ERROR: Matrix is still singular after preprocessing.\")\n",
    "    optimal_bandwidth = None\n",
    "\n",
    "# Hyperparameter tuning: Kernel & Fixed\n",
    "kernels = ['gaussian', 'bisquare', 'exponential']\n",
    "fixed_options = [True, False]\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "for kernel, fixed in product(kernels, fixed_options):\n",
    "    try:\n",
    "        print(f\"Testing: Kernel={kernel}, Fixed={fixed}\")\n",
    "\n",
    "        gwr_model = GWR(coords_train, train_data[target_variable].values.reshape(-1, 1), \n",
    "                        X_train_scaled, bw=optimal_bandwidth, kernel=kernel, fixed=fixed)\n",
    "        gwr_results = gwr_model.fit()\n",
    "\n",
    "        # Generate predictions\n",
    "        scale = gwr_results.scale\n",
    "        residuals = gwr_results.resid_response\n",
    "        y_test_pred = gwr_model.predict(coords_test, X_test_scaled, scale, residuals)\n",
    "\n",
    "        # Compute evaluation metrics\n",
    "        mae = mean_absolute_error(test_data[target_variable], y_test_pred.predictions.flatten())\n",
    "        mse = mean_squared_error(test_data[target_variable], y_test_pred.predictions.flatten())\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(test_data[target_variable], y_test_pred.predictions.flatten())\n",
    "        mape = np.mean(np.abs((test_data[target_variable] - y_test_pred.predictions.flatten()) / test_data[target_variable])) * 100\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"Kernel\": kernel,\n",
    "            \"Fixed\": fixed,\n",
    "            # \"AIC\": gwr_results.aic,\n",
    "            # \"AICc\": gwr_results.aicc,\n",
    "            \"R²\": r2,\n",
    "            \"MAE\": mae,\n",
    "            \"MSE\": mse,\n",
    "            \"RMSE\": rmse,\n",
    "            \"MAPE\": mape\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipping {kernel}, {fixed} due to error: {e}\")\n",
    "\n",
    "# Save results to Excel\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"gwr_vif_eval.csv\", index=False)\n",
    "print(\"✅ Experimentation results saved to gwr_vif_eval.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
