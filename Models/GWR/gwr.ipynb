{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)\n",
      "Collecting mgwr\n",
      "  Downloading mgwr-2.2.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: statsmodels in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.14.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: scipy>=0.11 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mgwr) (1.13.0)\n",
      "Collecting libpysal>=4.0.0 (from mgwr)\n",
      "  Downloading libpysal-4.12.1-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting spglm>=1.0.6 (from mgwr)\n",
      "  Downloading spglm-1.1.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting spreg (from mgwr)\n",
      "  Downloading spreg-1.8.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from statsmodels) (0.5.6)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from statsmodels) (23.2)\n",
      "Requirement already satisfied: beautifulsoup4>=4.10 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from libpysal>=4.0.0->mgwr) (4.12.3)\n",
      "Collecting geopandas>=0.10.0 (from libpysal>=4.0.0->mgwr)\n",
      "  Downloading geopandas-1.0.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: platformdirs>=2.0.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from libpysal>=4.0.0->mgwr) (3.11.0)\n",
      "Requirement already satisfied: requests>=2.27 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from libpysal>=4.0.0->mgwr) (2.31.0)\n",
      "Collecting shapely>=2.0.1 (from libpysal>=4.0.0->mgwr)\n",
      "  Downloading shapely-2.0.7-cp312-cp312-win_amd64.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: six in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from patsy>=0.5.6->statsmodels) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4>=4.10->libpysal>=4.0.0->mgwr) (2.5)\n",
      "Collecting pyogrio>=0.7.2 (from geopandas>=0.10.0->libpysal>=4.0.0->mgwr)\n",
      "  Downloading pyogrio-0.10.0-cp312-cp312-win_amd64.whl.metadata (5.6 kB)\n",
      "Collecting pyproj>=3.3.0 (from geopandas>=0.10.0->libpysal>=4.0.0->mgwr)\n",
      "  Downloading pyproj-3.7.1-cp312-cp312-win_amd64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.27->libpysal>=4.0.0->mgwr) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.27->libpysal>=4.0.0->mgwr) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.27->libpysal>=4.0.0->mgwr) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.27->libpysal>=4.0.0->mgwr) (2024.2.2)\n",
      "Downloading mgwr-2.2.1-py3-none-any.whl (47 kB)\n",
      "   ---------------------------------------- 0.0/47.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 47.9/47.9 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading libpysal-4.12.1-py3-none-any.whl (2.8 MB)\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.2/2.8 MB 3.5 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.3/2.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.5/2.8 MB 4.0 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.6/2.8 MB 3.8 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.7/2.8 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.0/2.8 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.3/2.8 MB 4.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.4/2.8 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.8/2.8 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 2.0/2.8 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 2.0/2.8 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.2/2.8 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.4/2.8 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.5/2.8 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.7/2.8 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.8/2.8 MB 3.8 MB/s eta 0:00:00\n",
      "Downloading spglm-1.1.0-py3-none-any.whl (41 kB)\n",
      "   ---------------------------------------- 0.0/41.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 41.4/41.4 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading spreg-1.8.2-py3-none-any.whl (388 kB)\n",
      "   ---------------------------------------- 0.0/388.2 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 112.6/388.2 kB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 256.0/388.2 kB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 358.4/388.2 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 388.2/388.2 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading geopandas-1.0.1-py3-none-any.whl (323 kB)\n",
      "   ---------------------------------------- 0.0/323.6 kB ? eta -:--:--\n",
      "   ------------------ --------------------- 153.6/323.6 kB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 323.6/323.6 kB 4.0 MB/s eta 0:00:00\n",
      "Downloading shapely-2.0.7-cp312-cp312-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.4 MB 10.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.8/1.4 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.2/1.4 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.4 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 7.7 MB/s eta 0:00:00\n",
      "Downloading pyogrio-0.10.0-cp312-cp312-win_amd64.whl (16.2 MB)\n",
      "   ---------------------------------------- 0.0/16.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/16.2 MB 4.8 MB/s eta 0:00:04\n",
      "    --------------------------------------- 0.3/16.2 MB 2.8 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.7/16.2 MB 5.0 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.9/16.2 MB 5.0 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 1.2/16.2 MB 5.3 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.4/16.2 MB 5.1 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.7/16.2 MB 5.2 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 2.0/16.2 MB 5.4 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 2.3/16.2 MB 5.5 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.6/16.2 MB 5.8 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 3.0/16.2 MB 5.8 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 3.4/16.2 MB 6.2 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 3.6/16.2 MB 6.0 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 4.0/16.2 MB 6.1 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 4.4/16.2 MB 6.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 4.5/16.2 MB 6.0 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 4.6/16.2 MB 5.8 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 4.8/16.2 MB 5.7 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 5.1/16.2 MB 5.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 5.4/16.2 MB 5.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 5.6/16.2 MB 5.7 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 5.9/16.2 MB 5.7 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 6.2/16.2 MB 5.7 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 6.5/16.2 MB 5.8 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 6.8/16.2 MB 5.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 6.9/16.2 MB 5.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 7.1/16.2 MB 5.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 7.2/16.2 MB 5.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 7.5/16.2 MB 5.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 7.5/16.2 MB 5.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 7.7/16.2 MB 5.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 7.8/16.2 MB 5.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 8.0/16.2 MB 5.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 8.2/16.2 MB 5.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 8.4/16.2 MB 5.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 8.7/16.2 MB 5.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 9.0/16.2 MB 5.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 9.3/16.2 MB 5.2 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 9.4/16.2 MB 5.2 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.7/16.2 MB 5.2 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.8/16.2 MB 5.2 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.9/16.2 MB 5.1 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 10.0/16.2 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 10.2/16.2 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 10.4/16.2 MB 5.0 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 10.7/16.2 MB 5.0 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 11.0/16.2 MB 5.0 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 11.1/16.2 MB 5.0 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 11.4/16.2 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 11.6/16.2 MB 5.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.8/16.2 MB 4.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 12.0/16.2 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 12.2/16.2 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 12.5/16.2 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.7/16.2 MB 4.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 13.0/16.2 MB 4.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 13.0/16.2 MB 4.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 13.2/16.2 MB 4.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.4/16.2 MB 4.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.6/16.2 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.8/16.2 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 14.1/16.2 MB 4.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.4/16.2 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.6/16.2 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.8/16.2 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 15.1/16.2 MB 4.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.4/16.2 MB 4.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.7/16.2 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.0/16.2 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.2/16.2 MB 4.6 MB/s eta 0:00:00\n",
      "Downloading pyproj-3.7.1-cp312-cp312-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.2/6.3 MB 2.8 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.4/6.3 MB 3.4 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.7/6.3 MB 3.8 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.9/6.3 MB 4.0 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.1/6.3 MB 3.8 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.4/6.3 MB 4.3 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.6/6.3 MB 4.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.8/6.3 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.0/6.3 MB 4.4 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.2/6.3 MB 4.3 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.4/6.3 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.5/6.3 MB 4.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 2.7/6.3 MB 4.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 2.8/6.3 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 2.9/6.3 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.1/6.3 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.2/6.3 MB 3.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.4/6.3 MB 3.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.4/6.3 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.5/6.3 MB 3.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.7/6.3 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.8/6.3 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.0/6.3 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.2/6.3 MB 3.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.4/6.3 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.5/6.3 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.5/6.3 MB 3.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.6/6.3 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.6/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.7/6.3 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.9/6.3 MB 3.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.1/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.3/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.5/6.3 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.8/6.3 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.9/6.3 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.3 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 3.5 MB/s eta 0:00:00\n",
      "Installing collected packages: shapely, pyproj, pyogrio, geopandas, libpysal, spreg, spglm, mgwr\n",
      "Successfully installed geopandas-1.0.1 libpysal-4.12.1 mgwr-2.2.1 pyogrio-0.10.0 pyproj-3.7.1 shapely-2.0.7 spglm-1.1.0 spreg-1.8.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy mgwr scikit-learn statsmodels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading new datasets...\n",
      "Defining target variable and features...\n",
      "Converting features and target variable to numeric...\n",
      "Checking for zero-variance columns...\n",
      "Initial Condition number of X_train: 1.51e+05\n",
      "Checking for multicollinearity using VIF...\n",
      "Dropping year due to high VIF (55.94)\n",
      "Dropping PreSchool_within_1km due to high VIF (20.27)\n",
      "Dropping lease_commence_date due to high VIF (12.15)\n",
      "Dropping flat_type_LE due to high VIF (9.53)\n",
      "Dropping Primary_within_1km due to high VIF (8.78)\n",
      "Dropping price_per_sqm due to high VIF (6.65)\n",
      "Dropping flat_model_LE due to high VIF (5.76)\n",
      "Applying jitter to geographical coordinates...\n",
      "Scaling features for numerical stability...\n",
      "Condition number after scaling: 2.01e+00\n",
      "Extracting geographical coordinates...\n",
      "Selecting optimal bandwidth using cross-validation...\n",
      "Optimal Bandwidth: 1434.0\n",
      "Fitting the GWR model...\n",
      "GWR Model Fitted Successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import cond, matrix_rank\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from mgwr.gwr import GWR\n",
    "from mgwr.sel_bw import Sel_BW\n",
    "\n",
    "# Load new datasets\n",
    "print(\"Loading new datasets...\")\n",
    "train_data = pd.read_csv(\"new_normalized_train.csv\").sample(50000, random_state=42)\n",
    "test_data = pd.read_csv(\"new_normalized_test.csv\").sample(50000, random_state=42)\n",
    "\n",
    "# Define target variable and features\n",
    "print(\"Defining target variable and features...\")\n",
    "target_variable = \"resale_price\"\n",
    "model_columns = [\n",
    "    \"month\", \"year\", \"town_LE\", \"flat_type_LE\", \"storey_range_LE\",  \n",
    "    \"price_per_sqm\", \"flat_model_LE\", \"lease_commence_date\", \"Latitude\", \"Longitude\", \n",
    "    \"LTAMRTStation_within_1km\", \"MallCoordinates_within_1km\", \"Hawker_within_1km\", \n",
    "    \"PreSchool_within_1km\", \"Primary_within_1km\", \"Secondary_within_1km\", \n",
    "    \"JuniorCollege_within_1km\", \"MixedLevel_within_1km\", \"NParks_within_1km\", \"Sports_within_1km\"\n",
    "]\n",
    "\n",
    "# Convert features and target variable to numeric\n",
    "print(\"Converting features and target variable to numeric...\")\n",
    "train_data[model_columns] = train_data[model_columns].apply(pd.to_numeric, errors='coerce')\n",
    "test_data[model_columns] = test_data[model_columns].apply(pd.to_numeric, errors='coerce')\n",
    "train_data[target_variable] = pd.to_numeric(train_data[target_variable], errors='coerce')\n",
    "test_data[target_variable] = pd.to_numeric(test_data[target_variable], errors='coerce')\n",
    "\n",
    "# Remove zero-variance columns\n",
    "print(\"Checking for zero-variance columns...\")\n",
    "zero_var_cols = [col for col in model_columns if train_data[col].nunique() == 1]\n",
    "if zero_var_cols:\n",
    "    print(f\"Dropping zero-variance columns: {zero_var_cols}\")\n",
    "    train_data.drop(columns=zero_var_cols, inplace=True)\n",
    "    test_data.drop(columns=zero_var_cols, inplace=True)\n",
    "    model_columns = [col for col in model_columns if col not in zero_var_cols]\n",
    "\n",
    "# Check condition number before processing\n",
    "cond_number = np.linalg.cond(train_data[model_columns].values)\n",
    "print(f\"Initial Condition number of X_train: {cond_number:.2e}\")\n",
    "\n",
    "# Remove highly collinear features using VIF (excluding Longitude and Latitude)\n",
    "print(\"Checking for multicollinearity using VIF...\")\n",
    "def calculate_vif(df):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = df.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "vif_columns = [col for col in model_columns if col not in [\"Longitude\", \"Latitude\"]]  # Keep these in the model\n",
    "while True:\n",
    "    vif_df = calculate_vif(train_data[vif_columns])\n",
    "    max_vif = vif_df[\"VIF\"].max()\n",
    "    if max_vif > 4:  # Lower VIF threshold to remove high collinearity\n",
    "        feature_to_drop = vif_df.loc[vif_df[\"VIF\"].idxmax(), \"Feature\"]\n",
    "        print(f\"Dropping {feature_to_drop} due to high VIF ({max_vif:.2f})\")\n",
    "        train_data.drop(columns=[feature_to_drop], inplace=True)\n",
    "        test_data.drop(columns=[feature_to_drop], inplace=True)\n",
    "        vif_columns.remove(feature_to_drop)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Check for low-variance features again after VIF filtering\n",
    "low_var_cols = [col for col in vif_columns if train_data[col].std() < 1e-4]\n",
    "if low_var_cols:\n",
    "    print(f\"Dropping low-variance columns: {low_var_cols}\")\n",
    "    train_data.drop(columns=low_var_cols, inplace=True)\n",
    "    test_data.drop(columns=low_var_cols, inplace=True)\n",
    "    vif_columns = [col for col in vif_columns if col not in low_var_cols]\n",
    "\n",
    "# Slightly jitter geographical coordinates to ensure uniqueness\n",
    "print(\"Applying jitter to geographical coordinates...\")\n",
    "train_data[[\"Longitude\", \"Latitude\"]] += np.random.normal(0, 0.0001, train_data[[\"Longitude\", \"Latitude\"]].shape)\n",
    "test_data[[\"Longitude\", \"Latitude\"]] += np.random.normal(0, 0.0001, test_data[[\"Longitude\", \"Latitude\"]].shape)\n",
    "\n",
    "# Ensure minimum spatial uniqueness threshold\n",
    "unique_locations = len(train_data[[\"Longitude\", \"Latitude\"]].drop_duplicates())\n",
    "if unique_locations / len(train_data) < 0.95:\n",
    "    print(\"❌ ERROR: Too many duplicate spatial points. GWR may fail.\")\n",
    "    exit()\n",
    "\n",
    "# Scale features for numerical stability\n",
    "print(\"Scaling features for numerical stability...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_data[vif_columns])\n",
    "X_test_scaled = scaler.transform(test_data[vif_columns])\n",
    "print(f\"Condition number after scaling: {cond(X_train_scaled):.2e}\")\n",
    "\n",
    "# Extract geographical coordinates\n",
    "print(\"Extracting geographical coordinates...\")\n",
    "coords_train = train_data[['Longitude', 'Latitude']].values\n",
    "coords_test = test_data[['Longitude', 'Latitude']].values\n",
    "\n",
    "# Check matrix rank before proceeding\n",
    "if matrix_rank(X_train_scaled) < X_train_scaled.shape[1]:\n",
    "    print(\"❌ ERROR: Feature matrix is still singular. Skipping GWR model.\")\n",
    "    exit()\n",
    "\n",
    "# Select optimal bandwidth using cross-validation\n",
    "print(\"Selecting optimal bandwidth using cross-validation...\")\n",
    "try:\n",
    "    selector = Sel_BW(coords_train, train_data[target_variable].values.reshape(-1, 1), X_train_scaled)\n",
    "    optimal_bandwidth = selector.search()\n",
    "    print(f\"Optimal Bandwidth: {optimal_bandwidth}\")\n",
    "except np.linalg.LinAlgError:\n",
    "    print(\"❌ ERROR: Matrix is still singular after preprocessing.\")\n",
    "    print(\"Possible cause: Check feature correlation or spatial diversity.\")\n",
    "    optimal_bandwidth = None\n",
    "\n",
    "# Fit the GWR model only if bandwidth selection was successful\n",
    "if optimal_bandwidth is not None:\n",
    "    print(\"Fitting the GWR model...\")\n",
    "    gwr_model = GWR(coords_train, train_data[target_variable].values.reshape(-1, 1), X_train_scaled, bw=optimal_bandwidth)\n",
    "    gwr_results = gwr_model.fit()\n",
    "    print(\"GWR Model Fitted Successfully!\")\n",
    "else:\n",
    "    print(\"Skipping GWR model fitting due to singular matrix issue.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading new datasets...\n",
      "Defining target variable and features...\n",
      "Converting features and target variable to numeric...\n",
      "Checking for zero-variance columns...\n",
      "Initial Condition number of X_train: 1.20e+05\n",
      "Checking for multicollinearity using VIF...\n",
      "Dropping year due to high VIF (60.30)\n",
      "Dropping PreSchool_within_1km due to high VIF (21.03)\n",
      "Dropping lease_commence_date due to high VIF (13.58)\n",
      "Dropping flat_type_LE due to high VIF (10.02)\n",
      "Dropping Primary_within_1km due to high VIF (9.21)\n",
      "Dropping price_per_sqm due to high VIF (6.55)\n",
      "Dropping flat_model_LE due to high VIF (5.61)\n",
      "Applying jitter to geographical coordinates...\n",
      "Scaling features for numerical stability...\n",
      "Condition number after scaling: 2.13e+00\n",
      "Extracting geographical coordinates...\n",
      "Selecting optimal bandwidth using cross-validation...\n",
      "Optimal Bandwidth: 301.0\n",
      "Fitting the GWR model...\n",
      "GWR Model Fitted Successfully!\n",
      "===========================================================================\n",
      "Model type                                                         Gaussian\n",
      "Number of observations:                                                1000\n",
      "Number of covariates:                                                    12\n",
      "\n",
      "Global Regression Results\n",
      "---------------------------------------------------------------------------\n",
      "Residual sum of squares:                                             13.176\n",
      "Log-likelihood:                                                     745.734\n",
      "AIC:                                                              -1467.468\n",
      "AICc:                                                             -1465.099\n",
      "BIC:                                                              -6811.686\n",
      "R2:                                                                   0.167\n",
      "Adj. R2:                                                              0.158\n",
      "\n",
      "Variable                              Est.         SE  t(Est/SE)    p-value\n",
      "------------------------------- ---------- ---------- ---------- ----------\n",
      "X0                                   0.266      0.004     72.949      0.000\n",
      "X1                                  -0.002      0.004     -0.421      0.674\n",
      "X2                                  -0.000      0.004     -0.061      0.951\n",
      "X3                                   0.047      0.004     12.831      0.000\n",
      "X4                                   0.002      0.004      0.551      0.582\n",
      "X5                                  -0.001      0.004     -0.176      0.860\n",
      "X6                                   0.004      0.005      0.860      0.390\n",
      "X7                                  -0.010      0.004     -2.505      0.012\n",
      "X8                                  -0.005      0.004     -1.383      0.167\n",
      "X9                                   0.001      0.004      0.371      0.711\n",
      "X10                                  0.006      0.004      1.583      0.113\n",
      "X11                                  0.010      0.004      2.361      0.018\n",
      "\n",
      "Geographically Weighted Regression (GWR) Results\n",
      "---------------------------------------------------------------------------\n",
      "Spatial kernel:                                           Adaptive bisquare\n",
      "Bandwidth used:                                                     301.000\n",
      "\n",
      "Diagnostic information\n",
      "---------------------------------------------------------------------------\n",
      "Residual sum of squares:                                              9.974\n",
      "Effective number of parameters (trace(S)):                           77.039\n",
      "Degree of freedom (n - trace(S)):                                   922.961\n",
      "Sigma estimate:                                                       0.104\n",
      "Log-likelihood:                                                     884.949\n",
      "AIC:                                                              -1613.820\n",
      "AICc:                                                             -1600.425\n",
      "BIC:                                                              -1230.823\n",
      "R2:                                                                   0.369\n",
      "Adjusted R2:                                                          0.317\n",
      "Adj. alpha (95%):                                                     0.008\n",
      "Adj. critical t value (95%):                                          2.667\n",
      "\n",
      "Summary Statistics For GWR Parameter Estimates\n",
      "---------------------------------------------------------------------------\n",
      "Variable                   Mean        STD        Min     Median        Max\n",
      "-------------------- ---------- ---------- ---------- ---------- ----------\n",
      "X0                   -22464637780.355 710038936019.582 -22464637780619.176      0.268      0.354\n",
      "X1                        0.000      0.008     -0.037     -0.001      0.028\n",
      "X2                       -0.010      0.016     -0.038     -0.014      0.119\n",
      "X3                        0.033      0.020      0.002      0.031      0.101\n",
      "X4                       -0.004      0.016     -0.033     -0.002      0.081\n",
      "X5                        0.005      0.013     -0.034      0.004      0.043\n",
      "X6                       -0.012      0.022     -0.068     -0.016      0.048\n",
      "X7                       -0.005      0.016     -0.052     -0.001      0.028\n",
      "X8                        0.002      0.012     -0.024      0.002      0.029\n",
      "X9                   -65654407059.678 2075136300389.179 -65654407059688.281     -0.001      0.058\n",
      "X10                      -0.003      0.013     -0.059     -0.002      0.029\n",
      "X11                       0.010      0.012     -0.017      0.013      0.045\n",
      "===========================================================================\n",
      "\n",
      "Evaluating GWR model...\n",
      "Mean R2 = 0.36933483911426856\n",
      "AIC = -1613.819695434233\n",
      "AICc = -1600.4247241336075\n",
      "Mean Absolute Error (MAE): 0.09\n",
      "Mean Squared Error (MSE): 0.01\n",
      "Root Mean Squared Error (RMSE): 0.11\n",
      "R² Score: 0.2903\n"
     ]
    }
   ],
   "source": [
    "# GWR VIF\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import cond, matrix_rank\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from mgwr.gwr import GWR\n",
    "from mgwr.sel_bw import Sel_BW\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load new datasets\n",
    "print(\"Loading new datasets...\")\n",
    "train_data = pd.read_csv(\"new_normalized_train.csv\")\n",
    "test_data = pd.read_csv(\"new_normalized_test.csv\")\n",
    "\n",
    "# Define target variable and features\n",
    "print(\"Defining target variable and features...\")\n",
    "target_variable = \"resale_price\"\n",
    "model_columns = [\n",
    "    \"month\", \"year\", \"town_LE\", \"flat_type_LE\", \"storey_range_LE\",  \n",
    "    \"price_per_sqm\", \"flat_model_LE\", \"lease_commence_date\", \"Latitude\", \"Longitude\", \n",
    "    \"LTAMRTStation_within_1km\", \"MallCoordinates_within_1km\", \"Hawker_within_1km\", \n",
    "    \"PreSchool_within_1km\", \"Primary_within_1km\", \"Secondary_within_1km\", \n",
    "    \"JuniorCollege_within_1km\", \"MixedLevel_within_1km\", \"NParks_within_1km\", \"Sports_within_1km\"\n",
    "]\n",
    "\n",
    "# Convert features and target variable to numeric\n",
    "print(\"Converting features and target variable to numeric...\")\n",
    "train_data[model_columns] = train_data[model_columns].apply(pd.to_numeric, errors='coerce')\n",
    "test_data[model_columns] = test_data[model_columns].apply(pd.to_numeric, errors='coerce')\n",
    "train_data[target_variable] = pd.to_numeric(train_data[target_variable], errors='coerce')\n",
    "test_data[target_variable] = pd.to_numeric(test_data[target_variable], errors='coerce')\n",
    "\n",
    "# Remove zero-variance columns\n",
    "print(\"Checking for zero-variance columns...\")\n",
    "zero_var_cols = [col for col in model_columns if train_data[col].nunique() == 1]\n",
    "if zero_var_cols:\n",
    "    print(f\"Dropping zero-variance columns: {zero_var_cols}\")\n",
    "    train_data.drop(columns=zero_var_cols, inplace=True)\n",
    "    test_data.drop(columns=zero_var_cols, inplace=True)\n",
    "    model_columns = [col for col in model_columns if col not in zero_var_cols]\n",
    "\n",
    "# Check condition number before processing\n",
    "cond_number = np.linalg.cond(train_data[model_columns].values)\n",
    "print(f\"Initial Condition number of X_train: {cond_number:.2e}\")\n",
    "\n",
    "# Remove highly collinear features using VIF (excluding Longitude and Latitude)\n",
    "print(\"Checking for multicollinearity using VIF...\")\n",
    "def calculate_vif(df):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = df.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "vif_columns = [col for col in model_columns if col not in [\"Longitude\", \"Latitude\"]]  # Keep these in the model\n",
    "while True:\n",
    "    vif_df = calculate_vif(train_data[vif_columns])\n",
    "    max_vif = vif_df[\"VIF\"].max()\n",
    "    if max_vif > 4:  # Lower VIF threshold to remove high collinearity\n",
    "        feature_to_drop = vif_df.loc[vif_df[\"VIF\"].idxmax(), \"Feature\"]\n",
    "        print(f\"Dropping {feature_to_drop} due to high VIF ({max_vif:.2f})\")\n",
    "        train_data.drop(columns=[feature_to_drop], inplace=True)\n",
    "        test_data.drop(columns=[feature_to_drop], inplace=True)\n",
    "        vif_columns.remove(feature_to_drop)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Check for low-variance features again after VIF filtering\n",
    "low_var_cols = [col for col in vif_columns if train_data[col].std() < 1e-4]\n",
    "if low_var_cols:\n",
    "    print(f\"Dropping low-variance columns: {low_var_cols}\")\n",
    "    train_data.drop(columns=low_var_cols, inplace=True)\n",
    "    test_data.drop(columns=low_var_cols, inplace=True)\n",
    "    vif_columns = [col for col in vif_columns if col not in low_var_cols]\n",
    "\n",
    "# Slightly jitter geographical coordinates to ensure uniqueness\n",
    "print(\"Applying jitter to geographical coordinates...\")\n",
    "train_data[[\"Longitude\", \"Latitude\"]] += np.random.normal(0, 0.0001, train_data[[\"Longitude\", \"Latitude\"]].shape)\n",
    "test_data[[\"Longitude\", \"Latitude\"]] += np.random.normal(0, 0.0001, test_data[[\"Longitude\", \"Latitude\"]].shape)\n",
    "\n",
    "# Ensure minimum spatial uniqueness threshold\n",
    "unique_locations = len(train_data[[\"Longitude\", \"Latitude\"]].drop_duplicates())\n",
    "if unique_locations / len(train_data) < 0.95:\n",
    "    print(\"❌ ERROR: Too many duplicate spatial points. GWR may fail.\")\n",
    "    exit()\n",
    "\n",
    "# Scale features for numerical stability\n",
    "print(\"Scaling features for numerical stability...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_data[vif_columns])\n",
    "X_test_scaled = scaler.transform(test_data[vif_columns])\n",
    "print(f\"Condition number after scaling: {cond(X_train_scaled):.2e}\")\n",
    "\n",
    "# Extract geographical coordinates\n",
    "print(\"Extracting geographical coordinates...\")\n",
    "coords_train = train_data[['Longitude', 'Latitude']].values\n",
    "coords_test = test_data[['Longitude', 'Latitude']].values\n",
    "\n",
    "# Check matrix rank before proceeding\n",
    "if matrix_rank(X_train_scaled) < X_train_scaled.shape[1]:\n",
    "    print(\"❌ ERROR: Feature matrix is still singular. Skipping GWR model.\")\n",
    "    exit()\n",
    "\n",
    "# Select optimal bandwidth using cross-validation\n",
    "print(\"Selecting optimal bandwidth using cross-validation...\")\n",
    "try:\n",
    "    selector = Sel_BW(coords_train, train_data[target_variable].values.reshape(-1, 1), X_train_scaled)\n",
    "    optimal_bandwidth = selector.search()\n",
    "    print(f\"Optimal Bandwidth: {optimal_bandwidth}\")\n",
    "except np.linalg.LinAlgError:\n",
    "    print(\"❌ ERROR: Matrix is still singular after preprocessing.\")\n",
    "    print(\"Possible cause: Check feature correlation or spatial diversity.\")\n",
    "    optimal_bandwidth = None\n",
    "\n",
    "# Fit the GWR model only if bandwidth selection was successful\n",
    "if optimal_bandwidth is not None:\n",
    "    print(\"Fitting the GWR model...\")\n",
    "    gwr_model = GWR(coords_train, train_data[target_variable].values.reshape(-1, 1), X_train_scaled, bw=optimal_bandwidth)\n",
    "    gwr_results = gwr_model.fit()\n",
    "    print(\"GWR Model Fitted Successfully!\")\n",
    "    gwr_results.summary()\n",
    "    \n",
    "    # Evaluate model performance\n",
    "    print(\"Evaluating GWR model...\")\n",
    "\n",
    "    print('Mean R2 =', gwr_results.R2)\n",
    "    print('AIC =', gwr_results.aic)\n",
    "    print('AICc =', gwr_results.aicc)\n",
    "\n",
    "    # Generate predictions correctly\n",
    "    scale = gwr_results.scale\n",
    "    residuals = gwr_results.resid_response\n",
    "\n",
    "    y_test_pred = gwr_model.predict(coords_test, X_test_scaled, scale, residuals)\n",
    "\n",
    "    # print(f\"y_test_pred: {y_test_pred.predictions}\")\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    mae = mean_absolute_error(test_data[target_variable], y_test_pred.predictions.flatten())\n",
    "    mse = mean_squared_error(test_data[target_variable], y_test_pred.predictions.flatten())\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(test_data[target_variable], y_test_pred.predictions.flatten())\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Skipping GWR model fitting due to singular matrix issue.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading new datasets...\n",
      "Defining target variable and features...\n",
      "Converting boolean columns to integers...\n",
      "Ensuring all features are numeric...\n",
      "Checking for zero-variance columns...\n",
      "Dropping zero-variance columns: ['flat_type_MULTI-GENERATION', 'flat_model_Type S2', 'flat_model_Terrace', 'flat_model_Premium Maisonette', 'flat_model_Multi Generation', 'flat_model_Model A-Maisonette', 'flat_model_Improved-Maisonette', 'flat_model_Adjoined flat', 'flat_model_3Gen']\n",
      "Initial Condition number of X_train: 1.58e+19\n",
      "Checking for multicollinearity using VIF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping flat_type_EXECUTIVE due to high VIF (inf)\n",
      "Dropping flat_model_Model A due to high VIF (727074.61)\n",
      "Dropping year due to high VIF (264.68)\n",
      "Dropping lease_commence_date due to high VIF (32.60)\n",
      "Dropping flat_type_4 ROOM due to high VIF (27.74)\n",
      "Dropping PreSchool_within_1km due to high VIF (24.47)\n",
      "Dropping Primary_within_1km due to high VIF (12.41)\n",
      "Dropping LTAMRTStation_within_1km due to high VIF (11.14)\n",
      "Dropping price_per_sqm due to high VIF (10.94)\n",
      "Dropping Hawker_within_1km due to high VIF (9.66)\n",
      "Dropping NParks_within_1km due to high VIF (5.28)\n",
      "Dropping month due to high VIF (4.61)\n",
      "Dropping Secondary_within_1km due to high VIF (4.52)\n",
      "Dropping storey_range_LE due to high VIF (4.22)\n",
      "Dropping low-variance columns: ['town_YISHUN', 'town_WOODLANDS', 'town_TOA PAYOH', 'town_TAMPINES', 'town_SERANGOON', 'town_SENGKANG', 'town_SEMBAWANG', 'town_QUEENSTOWN', 'town_PUNGGOL', 'town_PASIR RIS', 'town_MARINE PARADE', 'town_KALLANG/WHAMPOA', 'town_JURONG WEST', 'town_JURONG EAST', 'town_HOUGANG', 'town_GEYLANG', 'town_CLEMENTI', 'town_CHOA CHU KANG', 'town_CENTRAL AREA', 'town_BUKIT TIMAH', 'town_BUKIT PANJANG', 'town_BUKIT MERAH', 'town_BUKIT BATOK', 'town_BISHAN', 'town_BEDOK', 'flat_type_5 ROOM', 'flat_type_3 ROOM', 'flat_type_2 ROOM', 'flat_model_Type S1', 'flat_model_Standard', 'flat_model_Simplified', 'flat_model_Premium Apartment Loft', 'flat_model_Premium Apartment', 'flat_model_New Generation', 'flat_model_Model A2', 'flat_model_Maisonette', 'flat_model_Improved', 'flat_model_DBSS', 'flat_model_Apartment', 'JuniorCollege_within_1km', 'MixedLevel_within_1km', 'Sports_within_1km']\n",
      "Applying jitter to geographical coordinates...\n",
      "Scaling features for numerical stability...\n",
      "Condition number after scaling: 1.00e+00\n",
      "Extracting geographical coordinates...\n",
      "Selecting optimal bandwidth using cross-validation...\n",
      "Optimal Bandwidth: 45.0\n",
      "Fitting the GWR model...\n",
      "GWR Model Fitted Successfully!\n",
      "===========================================================================\n",
      "Model type                                                         Gaussian\n",
      "Number of observations:                                                1000\n",
      "Number of covariates:                                                     2\n",
      "\n",
      "Global Regression Results\n",
      "---------------------------------------------------------------------------\n",
      "Residual sum of squares:                                             16.051\n",
      "Log-likelihood:                                                     647.050\n",
      "AIC:                                                              -1290.100\n",
      "AICc:                                                             -1288.076\n",
      "BIC:                                                              -6877.889\n",
      "R2:                                                                   0.002\n",
      "Adj. R2:                                                              0.001\n",
      "\n",
      "Variable                              Est.         SE  t(Est/SE)    p-value\n",
      "------------------------------- ---------- ---------- ---------- ----------\n",
      "X0                                   0.262      0.004     65.324      0.000\n",
      "X1                                   0.006      0.004      1.448      0.148\n",
      "\n",
      "Geographically Weighted Regression (GWR) Results\n",
      "---------------------------------------------------------------------------\n",
      "Spatial kernel:                                           Adaptive bisquare\n",
      "Bandwidth used:                                                      45.000\n",
      "\n",
      "Diagnostic information\n",
      "---------------------------------------------------------------------------\n",
      "Residual sum of squares:                                             10.702\n",
      "Effective number of parameters (trace(S)):                           95.151\n",
      "Degree of freedom (n - trace(S)):                                   904.849\n",
      "Sigma estimate:                                                       0.109\n",
      "Log-likelihood:                                                     849.737\n",
      "AIC:                                                              -1507.172\n",
      "AICc:                                                             -1486.479\n",
      "BIC:                                                              -1035.285\n",
      "R2:                                                                   0.335\n",
      "Adjusted R2:                                                          0.265\n",
      "Adj. alpha (95%):                                                     0.001\n",
      "Adj. critical t value (95%):                                          3.286\n",
      "\n",
      "Summary Statistics For GWR Parameter Estimates\n",
      "---------------------------------------------------------------------------\n",
      "Variable                   Mean        STD        Min     Median        Max\n",
      "-------------------- ---------- ---------- ---------- ---------- ----------\n",
      "X0                        0.265      0.065      0.151      0.253      0.470\n",
      "X1                        0.009      0.043     -0.107      0.010      0.181\n",
      "===========================================================================\n",
      "\n",
      "Evaluating GWR model...\n",
      "Mean R2 = 0.33467179052459106\n",
      "AIC = -1507.1719152676928\n",
      "AICc = -1486.4791106000678\n",
      "Mean Absolute Error (MAE): 0.09\n",
      "Mean Squared Error (MSE): 0.01\n",
      "Root Mean Squared Error (RMSE): 0.11\n",
      "R² Score: 0.2109\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from numpy.linalg import cond, matrix_rank\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "# from mgwr.gwr import GWR\n",
    "# from mgwr.sel_bw import Sel_BW\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# # Load new datasets\n",
    "# print(\"Loading new datasets...\")\n",
    "# train_data = pd.read_csv(\"../../Data/normalized_test.csv\").sample(1000, random_state=42)\n",
    "# test_data = pd.read_csv(\"../../Data/normalized_train.csv\").sample(1000, random_state=42)\n",
    "\n",
    "# # Define target variable and features\n",
    "# print(\"Defining target variable and features...\")\n",
    "# target_variable = \"resale_price\"\n",
    "# model_columns = [\n",
    "#     \"month\", \"year\", \n",
    "\n",
    "#     \"town_YISHUN\", \"town_WOODLANDS\", \"town_TOA PAYOH\", \"town_TAMPINES\", \"town_SERANGOON\", \"town_SENGKANG\", \"town_SEMBAWANG\", \"town_QUEENSTOWN\", \"town_PUNGGOL\", \"town_PASIR RIS\", \n",
    "#     \"town_MARINE PARADE\", \"town_KALLANG/WHAMPOA\", \"town_JURONG WEST\", \"town_JURONG EAST\", \"town_HOUGANG\", \"town_GEYLANG\", \"town_CLEMENTI\", \"town_CHOA CHU KANG\", \"town_CENTRAL AREA\", \n",
    "#     \"town_BUKIT TIMAH\", \"town_BUKIT PANJANG\", \"town_BUKIT MERAH\", \"town_BUKIT BATOK\", \"town_BISHAN\", \"town_BEDOK\", \n",
    "\n",
    "#     \"flat_type_MULTI-GENERATION\", \"flat_type_EXECUTIVE\", \"flat_type_5 ROOM\", \"flat_type_4 ROOM\", \"flat_type_3 ROOM\", \"flat_type_2 ROOM\", \n",
    "\n",
    "#     \"storey_range_LE\", \n",
    "#     \"price_per_sqm\", \n",
    "\n",
    "#     \"flat_model_Type S2\", \"flat_model_Type S1\", \"flat_model_Terrace\", \"flat_model_Standard\", \"flat_model_Simplified\", \"flat_model_Premium Maisonette\", \"flat_model_Premium Apartment Loft\", \n",
    "#     \"flat_model_Premium Apartment\", \"flat_model_New Generation\", \"flat_model_Multi Generation\", \"flat_model_Model A2\", \"flat_model_Model A-Maisonette\", \"flat_model_Model A\", \n",
    "#     \"flat_model_Maisonette\", \"flat_model_Improved-Maisonette\", \"flat_model_Improved\", \"flat_model_DBSS\", \"flat_model_Apartment\", \"flat_model_Adjoined flat\", \"flat_model_3Gen\", \n",
    "\n",
    "#     \"lease_commence_date\",\n",
    "\n",
    "#     \"Latitude\", \"Longitude\", \n",
    "\n",
    "#     \"LTAMRTStation_within_1km\",\n",
    "#     \"MallCoordinates_within_1km\", \"Hawker_within_1km\", \n",
    "#     \"PreSchool_within_1km\", \"Primary_within_1km\", \"Secondary_within_1km\", \n",
    "#     \"JuniorCollege_within_1km\",\"MixedLevel_within_1km\", \n",
    "#     \"NParks_within_1km\", \"Sports_within_1km\", \n",
    "# ]\n",
    "\n",
    "\n",
    "# # Convert boolean columns to integers (0 or 1)\n",
    "# print(\"Converting boolean columns to integers...\")\n",
    "# bool_cols = train_data.select_dtypes(include=['bool']).columns\n",
    "# train_data[bool_cols] = train_data[bool_cols].astype(int)\n",
    "# test_data[bool_cols] = test_data[bool_cols].astype(int)\n",
    "\n",
    "# # Convert all features and target variable to numeric\n",
    "# print(\"Ensuring all features are numeric...\")\n",
    "# train_data[model_columns] = train_data[model_columns].apply(pd.to_numeric, errors='coerce')\n",
    "# test_data[model_columns] = test_data[model_columns].apply(pd.to_numeric, errors='coerce')\n",
    "# train_data[target_variable] = pd.to_numeric(train_data[target_variable], errors='coerce')\n",
    "# test_data[target_variable] = pd.to_numeric(test_data[target_variable], errors='coerce')\n",
    "\n",
    "# # Remove zero-variance columns\n",
    "# print(\"Checking for zero-variance columns...\")\n",
    "# zero_var_cols = [col for col in model_columns if train_data[col].nunique() == 1]\n",
    "# if zero_var_cols:\n",
    "#     print(f\"Dropping zero-variance columns: {zero_var_cols}\")\n",
    "#     train_data.drop(columns=zero_var_cols, inplace=True)\n",
    "#     test_data.drop(columns=zero_var_cols, inplace=True)\n",
    "#     model_columns = [col for col in model_columns if col not in zero_var_cols]\n",
    "\n",
    "# # Check condition number before processing\n",
    "# cond_number = np.linalg.cond(train_data[model_columns].values)\n",
    "# print(f\"Initial Condition number of X_train: {cond_number:.2e}\")\n",
    "\n",
    "# # Remove highly collinear features using VIF (excluding Longitude and Latitude)\n",
    "# print(\"Checking for multicollinearity using VIF...\")\n",
    "# def calculate_vif(df):\n",
    "#     vif_data = pd.DataFrame()\n",
    "#     vif_data[\"Feature\"] = df.columns\n",
    "#     vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "#     return vif_data\n",
    "\n",
    "# vif_columns = [col for col in model_columns if col not in [\"Longitude\", \"Latitude\"]]  # Keep these in the model\n",
    "# while True:\n",
    "#     vif_df = calculate_vif(train_data[vif_columns])\n",
    "#     max_vif = vif_df[\"VIF\"].max()\n",
    "#     if max_vif > 4:  # Lower VIF threshold to remove high collinearity\n",
    "#         feature_to_drop = vif_df.loc[vif_df[\"VIF\"].idxmax(), \"Feature\"]\n",
    "#         print(f\"Dropping {feature_to_drop} due to high VIF ({max_vif:.2f})\")\n",
    "#         train_data.drop(columns=[feature_to_drop], inplace=True)\n",
    "#         test_data.drop(columns=[feature_to_drop], inplace=True)\n",
    "#         vif_columns.remove(feature_to_drop)\n",
    "#     else:\n",
    "#         break\n",
    "\n",
    "# # Check for low-variance features again after VIF filtering\n",
    "# low_var_cols = [col for col in vif_columns if train_data[col].std() < 1]\n",
    "# if low_var_cols:\n",
    "#     print(f\"Dropping low-variance columns: {low_var_cols}\")\n",
    "#     train_data.drop(columns=low_var_cols, inplace=True)\n",
    "#     test_data.drop(columns=low_var_cols, inplace=True)\n",
    "#     vif_columns = [col for col in vif_columns if col not in low_var_cols]\n",
    "\n",
    "# # Slightly jitter geographical coordinates to ensure uniqueness\n",
    "# print(\"Applying jitter to geographical coordinates...\")\n",
    "# train_data[[\"Longitude\", \"Latitude\"]] += np.random.normal(0, 0.0001, train_data[[\"Longitude\", \"Latitude\"]].shape)\n",
    "# test_data[[\"Longitude\", \"Latitude\"]] += np.random.normal(0, 0.0001, test_data[[\"Longitude\", \"Latitude\"]].shape)\n",
    "\n",
    "# # Ensure minimum spatial uniqueness threshold\n",
    "# unique_locations = len(train_data[[\"Longitude\", \"Latitude\"]].drop_duplicates())\n",
    "# if unique_locations / len(train_data) < 0.95:\n",
    "#     print(\"❌ ERROR: Too many duplicate spatial points. GWR may fail.\")\n",
    "#     exit()\n",
    "\n",
    "# # Scale features for numerical stability\n",
    "# print(\"Scaling features for numerical stability...\")\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(train_data[vif_columns])\n",
    "# X_test_scaled = scaler.transform(test_data[vif_columns])\n",
    "# print(f\"Condition number after scaling: {cond(X_train_scaled):.2e}\")\n",
    "\n",
    "# # Extract geographical coordinates\n",
    "# print(\"Extracting geographical coordinates...\")\n",
    "# coords_train = train_data[['Longitude', 'Latitude']].values\n",
    "# coords_test = test_data[['Longitude', 'Latitude']].values\n",
    "\n",
    "# # Check matrix rank before proceeding\n",
    "# if matrix_rank(X_train_scaled) < X_train_scaled.shape[1]:\n",
    "#     print(\"❌ ERROR: Feature matrix is still singular. Skipping GWR model.\")\n",
    "#     exit()\n",
    "\n",
    "# # Select optimal bandwidth using cross-validation\n",
    "# print(\"Selecting optimal bandwidth using cross-validation...\")\n",
    "# try:\n",
    "#     selector = Sel_BW(coords_train, train_data[target_variable].values.reshape(-1, 1), X_train_scaled)\n",
    "#     optimal_bandwidth = selector.search()\n",
    "#     print(f\"Optimal Bandwidth: {optimal_bandwidth}\")\n",
    "# except np.linalg.LinAlgError:\n",
    "#     print(\"❌ ERROR: Matrix is still singular after preprocessing.\")\n",
    "#     print(\"Possible cause: Check feature correlation or spatial diversity.\")\n",
    "#     optimal_bandwidth = None\n",
    "\n",
    "# # Fit the GWR model only if bandwidth selection was successful\n",
    "# if optimal_bandwidth is not None:\n",
    "#     print(\"Fitting the GWR model...\")\n",
    "#     gwr_model = GWR(coords_train, train_data[target_variable].values.reshape(-1, 1), X_train_scaled, bw=optimal_bandwidth)\n",
    "#     gwr_results = gwr_model.fit()\n",
    "#     print(\"GWR Model Fitted Successfully!\")\n",
    "#     gwr_results.summary()\n",
    "    \n",
    "#     # Evaluate model performance\n",
    "#     print(\"Evaluating GWR model...\")\n",
    "\n",
    "#     print('Mean R2 =', gwr_results.R2)\n",
    "#     print('AIC =', gwr_results.aic)\n",
    "#     print('AICc =', gwr_results.aicc)\n",
    "\n",
    "#     # Generate predictions correctly\n",
    "#     scale = gwr_results.scale\n",
    "#     residuals = gwr_results.resid_response\n",
    "\n",
    "#     y_test_pred = gwr_model.predict(coords_test, X_test_scaled, scale, residuals)\n",
    "\n",
    "#     # print(f\"y_test_pred: {y_test_pred.predictions}\")\n",
    "\n",
    "#     # Compute evaluation metrics\n",
    "#     mae = mean_absolute_error(test_data[target_variable], y_test_pred.predictions.flatten())\n",
    "#     mse = mean_squared_error(test_data[target_variable], y_test_pred.predictions.flatten())\n",
    "#     rmse = np.sqrt(mse)\n",
    "#     r2 = r2_score(test_data[target_variable], y_test_pred.predictions.flatten())\n",
    "\n",
    "#     # Print evaluation metrics\n",
    "#     print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "#     print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "#     print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "#     print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# else:\n",
    "#     print(\"Skipping GWR model fitting due to singular matrix issue.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading new datasets...\n",
      "Defining target variable and features...\n",
      "Converting boolean columns to integers...\n",
      "Ensuring all features are numeric...\n",
      "Applying PCA to reduce feature dimensions...\n",
      "Number of Principal Components Selected: 7\n",
      "Explained Variance: 0.98\n",
      "Selecting optimal bandwidth using cross-validation...\n",
      "Optimal Bandwidth: 0.4\n",
      "Training the GWR model...\n"
     ]
    }
   ],
   "source": [
    "# GWR PCA\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mgwr.gwr import GWR\n",
    "from mgwr.sel_bw import Sel_BW\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load new datasets\n",
    "print(\"Loading new datasets...\")\n",
    "train_data = pd.read_csv(\"../../Data/normalized_test.csv\")\n",
    "test_data = pd.read_csv(\"../../Data/normalized_train.csv\")\n",
    "\n",
    "# Define target variable and features\n",
    "print(\"Defining target variable and features...\")\n",
    "target_variable = \"resale_price\"\n",
    "model_columns = [\n",
    "    \"month\", \"year\", \n",
    "    \"town_YISHUN\", \"town_WOODLANDS\", \"town_TOA PAYOH\", \"town_TAMPINES\", \"town_SERANGOON\", \"town_SENGKANG\", \"town_SEMBAWANG\", \"town_QUEENSTOWN\", \"town_PUNGGOL\", \"town_PASIR RIS\", \n",
    "    \"town_MARINE PARADE\", \"town_KALLANG/WHAMPOA\", \"town_JURONG WEST\", \"town_JURONG EAST\", \"town_HOUGANG\", \"town_GEYLANG\", \"town_CLEMENTI\", \"town_CHOA CHU KANG\", \"town_CENTRAL AREA\", \n",
    "    \"town_BUKIT TIMAH\", \"town_BUKIT PANJANG\", \"town_BUKIT MERAH\", \"town_BUKIT BATOK\", \"town_BISHAN\", \"town_BEDOK\", \n",
    "    \"flat_type_MULTI-GENERATION\", \"flat_type_EXECUTIVE\", \"flat_type_5 ROOM\", \"flat_type_4 ROOM\", \"flat_type_3 ROOM\", \"flat_type_2 ROOM\", \n",
    "    \"storey_range_LE\", \n",
    "    \"price_per_sqm\", \n",
    "    \"flat_model_Type S2\", \"flat_model_Type S1\", \"flat_model_Terrace\", \"flat_model_Standard\", \"flat_model_Simplified\", \"flat_model_Premium Maisonette\", \"flat_model_Premium Apartment Loft\", \n",
    "    \"flat_model_Premium Apartment\", \"flat_model_New Generation\", \"flat_model_Multi Generation\", \"flat_model_Model A2\", \"flat_model_Model A-Maisonette\", \"flat_model_Model A\", \n",
    "    \"flat_model_Maisonette\", \"flat_model_Improved-Maisonette\", \"flat_model_Improved\", \"flat_model_DBSS\", \"flat_model_Apartment\", \"flat_model_Adjoined flat\", \"flat_model_3Gen\", \n",
    "    \"lease_commence_date\",\n",
    "    \"Latitude\", \"Longitude\", \n",
    "    \"LTAMRTStation_within_1km\",\n",
    "    \"MallCoordinates_within_1km\", \"Hawker_within_1km\", \n",
    "    \"PreSchool_within_1km\", \"Primary_within_1km\", \"Secondary_within_1km\", \n",
    "    \"JuniorCollege_within_1km\",\"MixedLevel_within_1km\", \n",
    "    \"NParks_within_1km\", \"Sports_within_1km\", \n",
    "]\n",
    "\n",
    "# Convert boolean columns to integers (0 or 1)\n",
    "print(\"Converting boolean columns to integers...\")\n",
    "bool_cols = train_data.select_dtypes(include=['bool']).columns\n",
    "train_data[bool_cols] = train_data[bool_cols].astype(int)\n",
    "test_data[bool_cols] = test_data[bool_cols].astype(int)\n",
    "\n",
    "# Convert all features and target variable to numeric\n",
    "print(\"Ensuring all features are numeric...\")\n",
    "train_data[model_columns] = train_data[model_columns].apply(pd.to_numeric, errors='coerce')\n",
    "test_data[model_columns] = test_data[model_columns].apply(pd.to_numeric, errors='coerce')\n",
    "train_data[target_variable] = pd.to_numeric(train_data[target_variable], errors='coerce')\n",
    "test_data[target_variable] = pd.to_numeric(test_data[target_variable], errors='coerce')\n",
    "\n",
    "# Extract feature matrix and coordinates\n",
    "X_train = train_data[model_columns].values\n",
    "X_test = test_data[model_columns].values\n",
    "coords_train = train_data[['Longitude', 'Latitude']].values\n",
    "coords_test = test_data[['Longitude', 'Latitude']].values\n",
    "\n",
    "# Apply PCA for Dimensionality Reduction (Preserving 90% Variance)\n",
    "print(\"Applying PCA to reduce feature dimensions...\")\n",
    "pca = PCA(n_components=0.97)  # Preserve 90% variance\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "\n",
    "print(f\"Number of Principal Components Selected: {X_train_pca.shape[1]}\")\n",
    "print(f\"Explained Variance: {sum(pca.explained_variance_ratio_):.2f}\")\n",
    "\n",
    "# kernels = ['gaussian', 'bisquare', 'exponential']\n",
    "# adaptive_options = [True, False]\n",
    "\n",
    "kernel ='gaussian'\n",
    "adaptive = False\n",
    "\n",
    "try:\n",
    "        \n",
    "        # Select optimal bandwidth using cross-validation\n",
    "        print(\"Selecting optimal bandwidth using cross-validation...\")\n",
    "        # selector = Sel_BW(coords_train, train_data[target_variable].values.reshape(-1, 1), X_train_pca, kernel=kernel, fixed=not adaptive)\n",
    "        \n",
    "        # Randomly select sample_size rows for bandwidth selection\n",
    "        sample_indices = np.random.choice(len(train_data), 5000, replace=False)\n",
    "\n",
    "        selector = Sel_BW(\n",
    "            coords_train[sample_indices], \n",
    "            train_data[target_variable].values[sample_indices].reshape(-1, 1), \n",
    "            X_train_pca[sample_indices],\n",
    "            kernel=kernel, \n",
    "            fixed=not adaptive\n",
    "        )\n",
    "        \n",
    "        opt_bw = selector.search()\n",
    "        print(f\"Optimal Bandwidth: {opt_bw}\")\n",
    "        \n",
    "        print(\"Training the GWR model...\")\n",
    "        gwr_model = GWR(coords_train, train_data[target_variable].values.reshape(-1, 1), X_train_pca, bw=opt_bw, kernel=kernel)\n",
    "        gwr_results = gwr_model.fit()\n",
    "        print(\"GWR Model Fitted Successfully!\", )\n",
    "        gwr_results.summary()\n",
    "        \n",
    "        # Generate predictions correctly\n",
    "        scale = gwr_results.scale\n",
    "        residuals = gwr_results.resid_response\n",
    "        y_pred = gwr_model.predict(coords_test, X_test_pca, scale, residuals)\n",
    "        \n",
    "        mae = mean_absolute_error(test_data[target_variable], y_pred.predictions.flatten())\n",
    "        mse = mean_squared_error(test_data[target_variable], y_pred.predictions.flatten())\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(test_data[target_variable], y_pred.predictions.flatten())\n",
    "        mape = np.mean(np.abs((test_data[target_variable] - y_pred.predictions.flatten()) / test_data[target_variable])) * 100\n",
    "        aic = gwr_results.aic\n",
    "\n",
    "        # Print evaluation metrics\n",
    "\n",
    "        print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "        print(f\"Mean Absolute Percentage Error (MAPE): {mape}\")\n",
    "        print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "        print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "        print(f\"R² Score: {r2}\")\n",
    "\n",
    "except Exception as e:\n",
    "        print(f\"Error occurred with kernel={kernel}, Adaptive={adaptive} - {str(e)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for NaN or Inf in train data...\n",
      "month                 0\n",
      "year                  0\n",
      "town                  0\n",
      "town_LE               0\n",
      "town_YISHUN           0\n",
      "                     ..\n",
      "MixedLevel_nearest    0\n",
      "NParks_within_1km     0\n",
      "NParks_nearest        0\n",
      "Sports_within_1km     0\n",
      "Sports_nearest        0\n",
      "Length: 91, dtype: int64\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isinf' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChecking for NaN or Inf in train data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_data\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum())  \u001b[38;5;66;03m# Count NaNs\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum())  \u001b[38;5;66;03m# Count Inf values\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChecking for NaN or Inf in PCA-transformed training data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39misnan(X_train_pca)\u001b[38;5;241m.\u001b[39msum())  \u001b[38;5;66;03m# Count NaNs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py:2171\u001b[0m, in \u001b[0;36mNDFrame.__array_ufunc__\u001b[1;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2167\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   2168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array_ufunc__\u001b[39m(\n\u001b[0;32m   2169\u001b[0m     \u001b[38;5;28mself\u001b[39m, ufunc: np\u001b[38;5;241m.\u001b[39mufunc, method: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39minputs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m   2170\u001b[0m ):\n\u001b[1;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marraylike\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray_ufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mufunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arraylike.py:407\u001b[0m, in \u001b[0;36marray_ufunc\u001b[1;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__call__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs:\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;66;03m# for np.<ufunc>(..) calls\u001b[39;00m\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;66;03m# kwargs cannot necessarily be handled block-by-block, so only\u001b[39;00m\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;66;03m# take this path if there are no kwargs\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_mgr\n\u001b[1;32m--> 407\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mmgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mufunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;66;03m# otherwise specific ufunc methods (eg np.<ufunc>.accumulate(..))\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;66;03m# Those can have an axis keyword and thus can't be called block-by-block\u001b[39;00m\n\u001b[0;32m    411\u001b[0m     result \u001b[38;5;241m=\u001b[39m default_array_ufunc(inputs[\u001b[38;5;241m0\u001b[39m], ufunc, method, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:361\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    358\u001b[0m             kwargs[k] \u001b[38;5;241m=\u001b[39m obj[b\u001b[38;5;241m.\u001b[39mmgr_locs\u001b[38;5;241m.\u001b[39mindexer]\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(f):\n\u001b[1;32m--> 361\u001b[0m     applied \u001b[38;5;241m=\u001b[39m \u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    363\u001b[0m     applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:393\u001b[0m, in \u001b[0;36mBlock.apply\u001b[1;34m(self, func, **kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[0;32m    389\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;124;03m    apply the function to my values; return a block if we are not\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;124;03m    one\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 393\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m     result \u001b[38;5;241m=\u001b[39m maybe_coerce_values(result)\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_op_result(result)\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'isinf' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "print(\"Checking for NaN or Inf in train data...\")\n",
    "print(train_data.isna().sum())  # Count NaNs\n",
    "print(np.isinf(train_data).sum())  # Count Inf values\n",
    "\n",
    "print(\"Checking for NaN or Inf in PCA-transformed training data...\")\n",
    "print(np.isnan(X_train_pca).sum())  # Count NaNs\n",
    "print(np.isinf(X_train_pca).sum())  # Count Inf values\n",
    "\n",
    "print(\"Checking for NaN or Inf in X_train before PCA...\")\n",
    "print(np.isnan(X_train).sum())\n",
    "print(np.isinf(X_train).sum())\n",
    "\n",
    "print(\"Checking for NaN or Inf in target variable...\")\n",
    "print(np.isnan(train_data[target_variable]).sum())\n",
    "print(np.isinf(train_data[target_variable]).sum())\n",
    "\n",
    "print(\"Checking for NaN or Inf in coordinates...\")\n",
    "print(np.isnan(coords_train).sum())\n",
    "print(np.isinf(coords_train).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for NaN or Inf values in PCA-transformed data...\n",
      "NaN in X_train_pca: 0 | Inf in X_train_pca: 0\n",
      "NaN in X_test_pca: 0 | Inf in X_test_pca: 0\n"
     ]
    }
   ],
   "source": [
    "="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "GUASSIAN\n",
    "Mean Absolute Error (MAE): 0.07886822348806316\n",
    "Mean Squared Error (MSE): 0.010459145242201446\n",
    "Root Mean Squared Error (RMSE): 0.10226996256086851\n",
    "R² Score: 0.3386578854628215\n",
    "\n",
    "BISQUARE\n",
    "Mean Absolute Error (MAE): 0.07549681841499624\n",
    "Mean Squared Error (MSE): 0.00996194775705299\n",
    "Root Mean Squared Error (RMSE): 0.09980955744342819\n",
    "R² Score: 0.3700961749746552\n",
    "\n",
    "EXPONENTIAL\n",
    "Mean Absolute Error (MAE): 0.07836792864710952\n",
    "Mean Squared Error (MSE): 0.010350346436098199\n",
    "Root Mean Squared Error (RMSE): 0.1017366523731649\n",
    "R² Score: 0.34553734174927964"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading new datasets...\n",
      "Defining target variable and features...\n",
      "Converting boolean columns to integers...\n",
      "Ensuring all features are numeric...\n",
      "Testing PCA Variance = 0.95, Sample Size = 8000\n",
      "Applying PCA to reduce feature dimensions...\n",
      "Number of Principal Components Selected: 6\n",
      "Explained Variance: 0.96\n",
      "Selecting optimal bandwidth using cross-validation...\n",
      "Optimal Bandwidth: 56.0\n",
      "Testing PCA Variance = 0.95, Sample Size = 9000\n",
      "Applying PCA to reduce feature dimensions...\n",
      "Number of Principal Components Selected: 6\n",
      "Explained Variance: 0.96\n",
      "Selecting optimal bandwidth using cross-validation...\n",
      "❌ ERROR: Matrix is still singular after preprocessing.\n",
      "Testing PCA Variance = 0.95, Sample Size = 10000\n",
      "Applying PCA to reduce feature dimensions...\n",
      "Number of Principal Components Selected: 6\n",
      "Explained Variance: 0.96\n",
      "Selecting optimal bandwidth using cross-validation...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 90\u001b[0m\n\u001b[0;32m     82\u001b[0m sample_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlen\u001b[39m(train_data), sample_size, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     84\u001b[0m selector \u001b[38;5;241m=\u001b[39m Sel_BW(\n\u001b[0;32m     85\u001b[0m     coords_train[sample_indices], \n\u001b[0;32m     86\u001b[0m     train_data[target_variable]\u001b[38;5;241m.\u001b[39mvalues[sample_indices]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), \n\u001b[0;32m     87\u001b[0m     X_train_pca[sample_indices]\n\u001b[0;32m     88\u001b[0m )\n\u001b[1;32m---> 90\u001b[0m optimal_bandwidth \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimal Bandwidth: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimal_bandwidth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Store results\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mgwr\\sel_bw.py:324\u001b[0m, in \u001b[0;36mSel_BW.search\u001b[1;34m(self, search_method, criterion, bw_min, bw_max, interval, tol, max_iter, init_multi, tol_multi, rss_score, max_iter_multi, multi_bw_min, multi_bw_max, bws_same_times, verbose, pool)\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbw_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbw[\n\u001b[0;32m    322\u001b[0m         \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m#scalar, optimal bw from initial gwr model\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 324\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msel_hist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbw[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbw[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mgwr\\sel_bw.py:341\u001b[0m, in \u001b[0;36mSel_BW._bw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    338\u001b[0m     a, c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_section(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_glob, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_loc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoords,\n\u001b[0;32m    339\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstant)\n\u001b[0;32m    340\u001b[0m     delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.38197\u001b[39m  \u001b[38;5;66;03m#1 - (np.sqrt(5.0)-1.0)/2.0\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbw \u001b[38;5;241m=\u001b[39m \u001b[43mgolden_section\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgwr_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbw_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterval\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbw \u001b[38;5;241m=\u001b[39m equal_interval(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbw_min, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbw_max, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterval,\n\u001b[0;32m    346\u001b[0m                              gwr_func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mint_score, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mgwr\\search.py:66\u001b[0m, in \u001b[0;36mgolden_section\u001b[1;34m(a, c, delta, function, tol, max_iter, bw_max, int_score, verbose)\u001b[0m\n\u001b[0;32m     64\u001b[0m     score_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m[b]\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 66\u001b[0m     score_b \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28mdict\u001b[39m[b] \u001b[38;5;241m=\u001b[39m score_b\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mgwr\\sel_bw.py:333\u001b[0m, in \u001b[0;36mSel_BW._bw.<locals>.<lambda>\u001b[1;34m(bw)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_bw\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    330\u001b[0m     gwr_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m bw: getDiag[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion](\u001b[43mGWR\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_loc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\n\u001b[0;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\n\u001b[1;32m--> 333\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspherical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspherical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimized_function \u001b[38;5;241m=\u001b[39m gwr_func\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgolden_section\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mgwr\\gwr.py:348\u001b[0m, in \u001b[0;36mGWR.fit\u001b[1;34m(self, ini_params, tol, max_iter, solve, lite, pool)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    346\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoints\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 348\u001b[0m rslt \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_local_fit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    350\u001b[0m rslt_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mrslt))\n\u001b[0;32m    351\u001b[0m influ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(rslt_list[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mgwr.gwr import GWR\n",
    "from mgwr.sel_bw import Sel_BW\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from itertools import product\n",
    "\n",
    "# Load new datasets\n",
    "print(\"Loading new datasets...\")\n",
    "train_data = pd.read_csv(\"../../Data/normalized_test.csv\")\n",
    "test_data = pd.read_csv(\"../../Data/normalized_train.csv\")\n",
    "\n",
    "# Define target variable and features\n",
    "print(\"Defining target variable and features...\")\n",
    "target_variable = \"resale_price\"\n",
    "model_columns = [\n",
    "    \"month\", \"year\", \n",
    "    \"town_YISHUN\", \"town_WOODLANDS\", \"town_TOA PAYOH\", \"town_TAMPINES\", \"town_SERANGOON\", \"town_SENGKANG\", \"town_SEMBAWANG\", \"town_QUEENSTOWN\", \"town_PUNGGOL\", \"town_PASIR RIS\", \n",
    "    \"town_MARINE PARADE\", \"town_KALLANG/WHAMPOA\", \"town_JURONG WEST\", \"town_JURONG EAST\", \"town_HOUGANG\", \"town_GEYLANG\", \"town_CLEMENTI\", \"town_CHOA CHU KANG\", \"town_CENTRAL AREA\", \n",
    "    \"town_BUKIT TIMAH\", \"town_BUKIT PANJANG\", \"town_BUKIT MERAH\", \"town_BUKIT BATOK\", \"town_BISHAN\", \"town_BEDOK\", \n",
    "    \"flat_type_MULTI-GENERATION\", \"flat_type_EXECUTIVE\", \"flat_type_5 ROOM\", \"flat_type_4 ROOM\", \"flat_type_3 ROOM\", \"flat_type_2 ROOM\", \n",
    "    \"storey_range_LE\", \n",
    "    \"price_per_sqm\", \n",
    "    \"flat_model_Type S2\", \"flat_model_Type S1\", \"flat_model_Terrace\", \"flat_model_Standard\", \"flat_model_Simplified\", \"flat_model_Premium Maisonette\", \"flat_model_Premium Apartment Loft\", \n",
    "    \"flat_model_Premium Apartment\", \"flat_model_New Generation\", \"flat_model_Multi Generation\", \"flat_model_Model A2\", \"flat_model_Model A-Maisonette\", \"flat_model_Model A\", \n",
    "    \"flat_model_Maisonette\", \"flat_model_Improved-Maisonette\", \"flat_model_Improved\", \"flat_model_DBSS\", \"flat_model_Apartment\", \"flat_model_Adjoined flat\", \"flat_model_3Gen\", \n",
    "    \"lease_commence_date\",\n",
    "    \"Latitude\", \"Longitude\", \n",
    "    \"LTAMRTStation_within_1km\",\n",
    "    \"MallCoordinates_within_1km\", \"Hawker_within_1km\", \n",
    "    \"PreSchool_within_1km\", \"Primary_within_1km\", \"Secondary_within_1km\", \n",
    "    \"JuniorCollege_within_1km\",\"MixedLevel_within_1km\", \n",
    "    \"NParks_within_1km\", \"Sports_within_1km\", \n",
    "]\n",
    "\n",
    "# Convert boolean columns to integers (0 or 1)\n",
    "print(\"Converting boolean columns to integers...\")\n",
    "bool_cols = train_data.select_dtypes(include=['bool']).columns\n",
    "train_data[bool_cols] = train_data[bool_cols].astype(int)\n",
    "test_data[bool_cols] = test_data[bool_cols].astype(int)\n",
    "\n",
    "# Convert all features and target variable to numeric\n",
    "print(\"Ensuring all features are numeric...\")\n",
    "train_data[model_columns] = train_data[model_columns].apply(pd.to_numeric, errors='coerce')\n",
    "test_data[model_columns] = test_data[model_columns].apply(pd.to_numeric, errors='coerce')\n",
    "train_data[target_variable] = pd.to_numeric(train_data[target_variable], errors='coerce')\n",
    "test_data[target_variable] = pd.to_numeric(test_data[target_variable], errors='coerce')\n",
    "\n",
    "# Extract feature matrix and coordinates\n",
    "X_train = train_data[model_columns].values\n",
    "X_test = test_data[model_columns].values\n",
    "coords_train = train_data[['Longitude', 'Latitude']].values\n",
    "coords_test = test_data[['Longitude', 'Latitude']].values\n",
    "\n",
    "# Define PCA variance thresholds and sample sizes to test\n",
    "pca_variance_options = [0.95, 0.97, 0.99]\n",
    "sample_sizes = [8000, 9000, 10000]\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "for pca_variance, sample_size in product(pca_variance_options, sample_sizes):\n",
    "    print(f\"Testing PCA Variance = {pca_variance}, Sample Size = {sample_size}\")\n",
    "\n",
    "    # Apply PCA for Dimensionality Reduction\n",
    "    print(\"Applying PCA to reduce feature dimensions...\")\n",
    "    pca = PCA(n_components=pca_variance)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "\n",
    "    num_components = X_train_pca.shape[1]\n",
    "    explained_variance = sum(pca.explained_variance_ratio_)\n",
    "\n",
    "    print(f\"Number of Principal Components Selected: {num_components}\")\n",
    "    print(f\"Explained Variance: {explained_variance:.2f}\")\n",
    "\n",
    "    # Select optimal bandwidth using cross-validation\n",
    "    print(\"Selecting optimal bandwidth using cross-validation...\")\n",
    "    try:\n",
    "        # Randomly select sample_size rows for bandwidth selection\n",
    "        sample_indices = np.random.choice(len(train_data), sample_size, replace=False)\n",
    "\n",
    "        selector = Sel_BW(\n",
    "            coords_train[sample_indices], \n",
    "            train_data[target_variable].values[sample_indices].reshape(-1, 1), \n",
    "            X_train_pca[sample_indices]\n",
    "        )\n",
    "\n",
    "        optimal_bandwidth = selector.search()\n",
    "        print(f\"Optimal Bandwidth: {optimal_bandwidth}\")\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"PCA Variance\": pca_variance,\n",
    "            \"Sample Size\": sample_size,\n",
    "            \"Num Components\": num_components,\n",
    "            \"Explained Variance\": explained_variance,\n",
    "            \"Optimal Bandwidth\": optimal_bandwidth\n",
    "        })\n",
    "\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"❌ ERROR: Matrix is still singular after preprocessing.\")\n",
    "        results.append({\n",
    "            \"PCA Variance\": pca_variance,\n",
    "            \"Sample Size\": sample_size,\n",
    "            \"Num Components\": num_components,\n",
    "            \"Explained Variance\": explained_variance,\n",
    "            \"Optimal Bandwidth\": \"Singular Matrix Error\"\n",
    "        })\n",
    "\n",
    "# Save results to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"gwr_pca_bandwidth_experiment.csv\", index=False)\n",
    "print(\"✅ Experimentation results saved to gwr_pca_bandwidth_experiment.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\distributed\\node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 59812 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading new datasets...\n",
      "Defining target variable and features...\n",
      "Converting boolean columns to integers...\n",
      "Ensuring all features are numeric...\n",
      "Applying PCA for 0.97 variance...\n",
      "Number of Principal Components Selected: 7\n",
      "Explained Variance: 0.98\n",
      "Traning and Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\distributed\\client.py:3370: UserWarning: Sending large graph of size 234.50 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "2025-03-12 20:47:18,237 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:59832 (pid=5740) exceeded 95% memory budget. Restarting...\n",
      "2025-03-12 20:47:18,773 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-03-12 20:57:54,976 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:59838 (pid=7944) exceeded 95% memory budget. Restarting...\n",
      "2025-03-12 20:57:55,123 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:59838 (pid=7944) is slow to terminate; trying again\n",
      "2025-03-12 20:57:55,663 - distributed.nanny - WARNING - Restarting worker\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-9136864' coro=<Client._gather.<locals>.wait() done, defined at c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\distributed\\client.py:2394> exception=AllExit()>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\distributed\\client.py\", line 2403, in wait\n",
      "    raise AllExit()\n",
      "distributed.client.AllExit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 133\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKernel\u001b[39m\u001b[38;5;124m\"\u001b[39m: kernel,\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdaptive\u001b[39m\u001b[38;5;124m\"\u001b[39m: adaptive,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAIC\u001b[39m\u001b[38;5;124m\"\u001b[39m: aic\n\u001b[0;32m    130\u001b[0m     }\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Parallel grid search using Dask\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mdask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtrain_gwr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madaptive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madaptive\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madaptive_options\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Save results to CSV\u001b[39;00m\n\u001b[0;32m    136\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dask\\base.py:662\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    659\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 662\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mgwr.gwr import GWR\n",
    "from mgwr.sel_bw import Sel_BW\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from itertools import product\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Initialize Dask client for parallel processing\n",
    "client = Client()\n",
    "\n",
    "# Load new datasets\n",
    "print(\"Loading new datasets...\")\n",
    "train_data = pd.read_csv(\"../../Data/normalized_test.csv\")\n",
    "test_data = pd.read_csv(\"../../Data/normalized_train.csv\")\n",
    "\n",
    "# Define target variable and features\n",
    "print(\"Defining target variable and features...\")\n",
    "target_variable = \"resale_price\"\n",
    "model_columns = [\n",
    "    \"month\", \"year\", \n",
    "    \"town_YISHUN\", \"town_WOODLANDS\", \"town_TOA PAYOH\", \"town_TAMPINES\", \"town_SERANGOON\", \"town_SENGKANG\", \"town_SEMBAWANG\", \"town_QUEENSTOWN\", \"town_PUNGGOL\", \"town_PASIR RIS\", \n",
    "    \"town_MARINE PARADE\", \"town_KALLANG/WHAMPOA\", \"town_JURONG WEST\", \"town_JURONG EAST\", \"town_HOUGANG\", \"town_GEYLANG\", \"town_CLEMENTI\", \"town_CHOA CHU KANG\", \"town_CENTRAL AREA\", \n",
    "    \"town_BUKIT TIMAH\", \"town_BUKIT PANJANG\", \"town_BUKIT MERAH\", \"town_BUKIT BATOK\", \"town_BISHAN\", \"town_BEDOK\", \n",
    "    \"flat_type_MULTI-GENERATION\", \"flat_type_EXECUTIVE\", \"flat_type_5 ROOM\", \"flat_type_4 ROOM\", \"flat_type_3 ROOM\", \"flat_type_2 ROOM\", \n",
    "    \"storey_range_LE\", \n",
    "    \"price_per_sqm\", \n",
    "    \"flat_model_Type S2\", \"flat_model_Type S1\", \"flat_model_Terrace\", \"flat_model_Standard\", \"flat_model_Simplified\", \"flat_model_Premium Maisonette\", \"flat_model_Premium Apartment Loft\", \n",
    "    \"flat_model_Premium Apartment\", \"flat_model_New Generation\", \"flat_model_Multi Generation\", \"flat_model_Model A2\", \"flat_model_Model A-Maisonette\", \"flat_model_Model A\", \n",
    "    \"flat_model_Maisonette\", \"flat_model_Improved-Maisonette\", \"flat_model_Improved\", \"flat_model_DBSS\", \"flat_model_Apartment\", \"flat_model_Adjoined flat\", \"flat_model_3Gen\", \n",
    "    \"lease_commence_date\",\n",
    "    \"Latitude\", \"Longitude\", \n",
    "    \"LTAMRTStation_within_1km\",\n",
    "    \"MallCoordinates_within_1km\", \"Hawker_within_1km\", \n",
    "    \"PreSchool_within_1km\", \"Primary_within_1km\", \"Secondary_within_1km\", \n",
    "    \"JuniorCollege_within_1km\",\"MixedLevel_within_1km\", \n",
    "    \"NParks_within_1km\", \"Sports_within_1km\", \n",
    "]\n",
    "\n",
    "# Convert boolean columns to integers (0 or 1)\n",
    "print(\"Converting boolean columns to integers...\")\n",
    "bool_cols = train_data.select_dtypes(include=['bool']).columns\n",
    "train_data[bool_cols] = train_data[bool_cols].astype(int)\n",
    "test_data[bool_cols] = test_data[bool_cols].astype(int)\n",
    "\n",
    "# Convert all features and target variable to numeric\n",
    "print(\"Ensuring all features are numeric...\")\n",
    "train_data[model_columns] = train_data[model_columns].apply(pd.to_numeric, errors='coerce')\n",
    "test_data[model_columns] = test_data[model_columns].apply(pd.to_numeric, errors='coerce')\n",
    "train_data[target_variable] = pd.to_numeric(train_data[target_variable], errors='coerce')\n",
    "test_data[target_variable] = pd.to_numeric(test_data[target_variable], errors='coerce')\n",
    "\n",
    "# Extract feature matrix and coordinates\n",
    "X_train = train_data[model_columns].values\n",
    "X_test = test_data[model_columns].values\n",
    "coords_train = train_data[['Longitude', 'Latitude']].values\n",
    "coords_test = test_data[['Longitude', 'Latitude']].values\n",
    "\n",
    "# Apply PCA to retain 97% variance\n",
    "print(\"Applying PCA for 0.97 variance...\")\n",
    "pca = PCA(n_components=0.97)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "print(f\"Number of Principal Components Selected: {X_train_pca.shape[1]}\")\n",
    "print(f\"Explained Variance: {sum(pca.explained_variance_ratio_):.2f}\")\n",
    "\n",
    "\n",
    "# kernels = ['gaussian', 'bisquare', 'exponential']\n",
    "# adaptive_options = [True, False]\n",
    "\n",
    "# Define hyperparameter grid\n",
    "kernels = ['gaussian']\n",
    "adaptive_options = [False]\n",
    "\n",
    "print(\"Traning and Evaluation...\")\n",
    "# Function to train and evaluate a GWR model\n",
    "@dask.delayed\n",
    "def train_gwr(kernel, adaptive):\n",
    "    print(f\"Testing kernel: {kernel}, Adaptive: {adaptive}\", flush=True)\n",
    "    try:\n",
    "        # selector = Sel_BW(coords_train, train_data[target_variable].values.reshape(-1, 1), X_train_pca, kernel=kernel, fixed=not adaptive)\n",
    "        \n",
    "        # Randomly select sample_size rows for bandwidth selection\n",
    "        sample_indices = np.random.choice(len(train_data), 5000, replace=False)\n",
    "\n",
    "        selector = Sel_BW(\n",
    "            coords_train[sample_indices], \n",
    "            train_data[target_variable].values[sample_indices].reshape(-1, 1), \n",
    "            X_train_pca[sample_indices],\n",
    "            kernel=kernel, \n",
    "            fixed=not adaptive\n",
    "        )\n",
    "        \n",
    "        opt_bw = selector.search()\n",
    "        \n",
    "        print(\"Fitting the GWR model...\", flush=True)\n",
    "        gwr_model = GWR(coords_train, train_data[target_variable].values.reshape(-1, 1), X_train_pca, bw=opt_bw, kernel=kernel, fixed=not adaptive)\n",
    "        gwr_results = gwr_model.fit()\n",
    "        print(\"GWR Model Fitted Successfully!\", flush=True)\n",
    "        gwr_results.summary()\n",
    "        \n",
    "        # Generate predictions correctly\n",
    "        scale = gwr_results.scale\n",
    "        residuals = gwr_results.resid_response\n",
    "        y_pred = gwr_model.predict(coords_test, X_test_pca, scale, residuals)\n",
    "        \n",
    "        mae = mean_absolute_error(test_data[target_variable], y_pred.predictions.flatten())\n",
    "        mse = mean_squared_error(test_data[target_variable], y_pred.predictions.flatten())\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(test_data[target_variable], y_pred.predictions.flatten())\n",
    "        mape = np.mean(np.abs((test_data[target_variable] - y_pred.predictions.flatten()) / test_data[target_variable])) * 100\n",
    "        aic = gwr_results.aic\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred with kernel: {kernel}, Adaptive: {adaptive}: {str(e)}\", flush=True)\n",
    "        opt_bw, mae, mse, rmse, r2, mape, aic = -1, -1, -1, -1, -1, -1, -1\n",
    "    \n",
    "    return {\n",
    "        \"Kernel\": kernel,\n",
    "        \"Adaptive\": adaptive,\n",
    "        \"Bandwidth\": opt_bw,\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R²\": r2,\n",
    "        \"MAPE\": mape,\n",
    "        \"AIC\": aic\n",
    "    }\n",
    "\n",
    "# Parallel grid search using Dask\n",
    "results = dask.compute(*[train_gwr(kernel, adaptive) for kernel, adaptive in product(kernels, adaptive_options)])\n",
    "\n",
    "# Save results to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"gwr_results_1.csv\", index=False)\n",
    "\n",
    "# Identify best model based on AIC\n",
    "best_result = min(results, key=lambda x: x[\"AIC\"])\n",
    "print(f\"Best Kernel: {best_result['Kernel']}, Adaptive: {best_result['Adaptive']}, Optimal Bandwidth: {best_result['Bandwidth']}\")\n",
    "print(f\"Best AIC Score: {best_result['AIC']}\")\n",
    "\n",
    "# Shutdown the Dask client to free resources\n",
    "client.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
