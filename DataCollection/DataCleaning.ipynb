{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All values in the 'coordinates' column are valid 2D arrays.\n"
     ]
    }
   ],
   "source": [
    "# CHECK IF COORDINATES ARE STORED IN A 2D ARRAY\n",
    "\n",
    "import pandas as pd\n",
    "import ast  # To safely evaluate string representation of lists\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = \"LTAMRTStation.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "def is_valid_2d_array(coord_str):\n",
    "    \"\"\"\n",
    "    Check if a given string representation of coordinates is a valid 2D array.\n",
    "    - Must be a list of lists.\n",
    "    - Each inner list must contain exactly 2 elements (longitude, latitude).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        coordinates = ast.literal_eval(coord_str)  # Convert string back to list\n",
    "        if isinstance(coordinates, list) and all(\n",
    "            isinstance(coord, list) and len(coord) == 2 for coord in coordinates\n",
    "        ):\n",
    "            return True\n",
    "        return False\n",
    "    except (SyntaxError, ValueError):\n",
    "        return False\n",
    "\n",
    "# Apply the function to check each row in the \"coordinates\" column\n",
    "df[\"is_2D_valid\"] = df[\"coordinates\"].apply(is_valid_2d_array)\n",
    "\n",
    "# Print the rows where coordinates are NOT valid 2D arrays\n",
    "invalid_rows = df[~df[\"is_2D_valid\"]]\n",
    "\n",
    "# Display results\n",
    "if invalid_rows.empty:\n",
    "    print(\"✅ All values in the 'coordinates' column are valid 2D arrays.\")\n",
    "else:\n",
    "    print(f\"❌ Found {len(invalid_rows)} rows with invalid coordinates.\")\n",
    "    print(\"Here are the invalid rows:\")\n",
    "    print(invalid_rows[[\"geometry_type\", \"coordinates\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "month                  0\n",
       "town                   0\n",
       "flat_type              0\n",
       "block                  0\n",
       "street_name            0\n",
       "storey_range           0\n",
       "floor_area_sqm         0\n",
       "flat_model             0\n",
       "lease_commence_date    0\n",
       "remaining_lease        0\n",
       "resale_price           0\n",
       "Latitude               0\n",
       "Longitude              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECK IF NULL\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('HDBResale_with_coordinates_GoogleMaps.csv')\n",
    "\n",
    "df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'NParksTracks.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the cleaned CSV file\u001b[39;00m\n\u001b[0;32m      4\u001b[0m cleaned_csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNParksTracks.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m df_cleaned \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned_csv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Get the number of rows\u001b[39;00m\n\u001b[0;32m      8\u001b[0m num_rows \u001b[38;5;241m=\u001b[39m df_cleaned\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'NParksTracks.csv'"
     ]
    }
   ],
   "source": [
    "# GET TL NO. OF ROWS\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned CSV file\n",
    "cleaned_csv_path = \"NParksTracks.csv\"\n",
    "df_cleaned = pd.read_csv(cleaned_csv_path)\n",
    "\n",
    "# Get the number of rows\n",
    "num_rows = df_cleaned.shape[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total number of rows: {num_rows}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning for SportCenters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset:\n",
      "- Name\n",
      "- Latitude\n",
      "- Longitude\n",
      "- SPORTS_CEN\n",
      "- FACILITIES\n",
      "- HOUSE_BLOC\n",
      "- ROAD_NAME\n",
      "- POSTAL_COD\n",
      "- INFORMATIO\n",
      "- CONTACT_NO\n",
      "- STADIUM_OP\n",
      "- BOOKING_LI\n",
      "- ATHLETICS_\n",
      "- FOOTBALL_F\n",
      "- FACILITY_I\n",
      "- STATUS\n",
      "- TIER\n",
      "- INC_CRC\n",
      "- FMEL_UPD_D\n",
      "- MAINTENANC\n",
      "- SWIMMING_C\n",
      "- SPORTS_HAL\n",
      "- GYM_OPERAT\n",
      "- COMPETITIO\n",
      "- TEACHING_P\n",
      "- WADING_POO\n",
      "- INDOOR_SPO\n",
      "- BADMINTON_\n",
      "- TABLE_TENN\n",
      "- GYM\n",
      "- PICKLEBALL\n",
      "- TENNIS_SQU\n",
      "- TENNIS_COU\n",
      "- FOOTBALL_S\n",
      "- OTHERS_OPE\n",
      "- NETBALL_CO\n",
      "- SQUASH_COU\n",
      "- LAWN_BOWL_\n",
      "- RUGBY_FIEL\n",
      "- PETANQUE_C\n",
      "- GATEBALL_C\n",
      "- ACTIVE_HEA\n",
      "- SOCCER_COU\n",
      "- UPGRADING_\n",
      "- HOCKEY_PIT\n",
      "- VOLLEYBALL\n",
      "- BASKETBALL\n",
      "✅ Cleaned CSV file saved: SportSGSportFacilities.csv\n",
      "Columns in the dataset:\n",
      "['SPORTS_CEN', 'Latitude', 'Longitude']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "csv_file_path = \"UncleanedData/SportSGSportFacilities_Full.xlsx\"  # Input CSV file\n",
    "csv_output_path = \"SportSGSportFacilities.csv\"  # Output CSV file\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_excel(csv_file_path)\n",
    "\n",
    "# Print all column names, each on a new line\n",
    "print(\"Columns in the dataset:\")\n",
    "for col in df.columns:\n",
    "    print(f\"- {col}\")  # Print each column separately\n",
    "\n",
    "# Columns to keep\n",
    "columns_to_keep = [\"SPORTS_CEN\", \"Latitude\", \"Longitude\"]\n",
    "\n",
    "# Keep only the specified columns\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df.to_csv(csv_output_path, index=False)\n",
    "\n",
    "print(f\"✅ Cleaned CSV file saved: {csv_output_path}\")\n",
    "\n",
    "print(\"Columns in the dataset:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning for LTAMRTStation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned CSV file saved: LTAMRTStation.csv\n",
      "Columns in the dataset:\n",
      "['Latitude', 'Longitude', 'STATION_NA']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "excel_file_path = \"UncleanedData/LTAMRTStationExit_Full.xlsx\"  # Input Excel file\n",
    "csv_output_path = \"LTAMRTStation.csv\"  # Output CSV file\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Columns to drop\n",
    "columns_to_drop = [\"Name\", \"EXIT_CODE\", \"INC_CRC\", \"FMEL_UPD_D\"]\n",
    "\n",
    "# Drop the specified columns\n",
    "df = df.drop(columns=columns_to_drop, errors=\"ignore\")  # Ignore if column not found\n",
    "\n",
    "# Save the cleaned DataFrame to CSV\n",
    "df.to_csv(csv_output_path, index=False)\n",
    "\n",
    "print(f\"✅ Cleaned CSV file saved: {csv_output_path}\")\n",
    "\n",
    "print(\"Columns in the dataset:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Updated CSV saved as: LTAMRTStation.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "csv_file_path = \"LTAMRTStation.csv\"  # Input CSV file\n",
    "updated_csv_path = \"LTAMRTStation.csv\"  # Output CSV file\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Ensure 'Longitude' and 'Latitude' columns exist\n",
    "if \"Longitude\" in df.columns and \"Latitude\" in df.columns:\n",
    "    # Create a new 'coordinates' column with [[Longitude, Latitude]] format (2D array)\n",
    "    df[\"coordinates\"] = df.apply(lambda row: [[row[\"Longitude\"], row[\"Latitude\"]]], axis=1)\n",
    "\n",
    "    # Drop the original longitude and latitude columns\n",
    "    df = df.drop(columns=[\"Longitude\", \"Latitude\"], errors=\"ignore\")\n",
    "\n",
    "    # Save the updated DataFrame to the same CSV file\n",
    "    df.to_csv(updated_csv_path, index=False)\n",
    "\n",
    "    print(f\"✅ Updated CSV saved as: {updated_csv_path}\")\n",
    "else:\n",
    "    print(\"❌ 'Longitude' or 'Latitude' columns not found in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Updated CSV saved as: LTAMRTStation.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast  # To safely convert string representations of lists\n",
    "\n",
    "# Define file paths\n",
    "csv_file_path = \"LTAMRTStation.csv\"  # Input CSV file\n",
    "updated_csv_path = \"LTAMRTStation.csv\"  # Output CSV file\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Ensure required columns exist\n",
    "if \"STATION_NA\" in df.columns and \"coordinates\" in df.columns:\n",
    "    # Convert 'coordinates' column from string to actual lists\n",
    "    df[\"coordinates\"] = df[\"coordinates\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "    # Group by 'STATION_NA' and merge coordinates into a single 2D array per station\n",
    "    df_grouped = df.groupby(\"STATION_NA\", as_index=False).agg({\n",
    "        \"coordinates\": lambda x: [coord for sublist in x for coord in sublist]  # Flatten to single 2D array\n",
    "    })\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    df_grouped.to_csv(updated_csv_path, index=False)\n",
    "\n",
    "    print(f\"✅ Updated CSV saved as: {updated_csv_path}\")\n",
    "else:\n",
    "    print(\"❌ Required columns ('STATION_NA', 'coordinates') not found in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning for NPARKS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path_geojson = \"UncleanedData/NParksTracks.geojson\"  # Input GeoJSON file\n",
    "csv_output_path = \"NParksTracks.csv\"  # Output CSV file\n",
    "\n",
    "# Load the GeoJSON file\n",
    "with open(file_path_geojson, \"r\", encoding=\"utf-8\") as file:\n",
    "    geojson_data = json.load(file)\n",
    "\n",
    "# Extract relevant data from the GeoJSON\n",
    "features = geojson_data.get(\"features\", [])\n",
    "\n",
    "geojson_data_list = []\n",
    "\n",
    "def flatten_multiline_coordinates(multi_coords):\n",
    "    \"\"\"Flatten a 3D MultiLineString coordinates array into 2D.\"\"\"\n",
    "    return [coord for line in multi_coords for coord in line]\n",
    "\n",
    "for feature in features:\n",
    "    properties = feature.get(\"properties\", {})  # Extract properties\n",
    "    geometry = feature.get(\"geometry\", {})  # Extract geometry\n",
    "    coordinates = geometry.get(\"coordinates\", [])\n",
    "    geom_type = geometry.get(\"type\")\n",
    "\n",
    "    # Process coordinates based on geometry type\n",
    "    if geom_type == \"MultiLineString\":\n",
    "        coordinates = flatten_multiline_coordinates(coordinates)  # Flatten 3D to 2D\n",
    "    elif geom_type == \"LineString\":\n",
    "        pass  # Already 2D\n",
    "\n",
    "    # Store the processed data\n",
    "    properties[\"geometry_type\"] = geom_type\n",
    "    properties[\"coordinates\"] = json.dumps(coordinates)  # Store as JSON string for CSV compatibility\n",
    "    geojson_data_list.append(properties)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_geojson = pd.DataFrame(geojson_data_list)\n",
    "\n",
    "# Save the cleaned data to a CSV file\n",
    "df_geojson.to_csv(csv_output_path, index=False)\n",
    "\n",
    "print(f\"CSV file saved: {csv_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = \"NParksTracks.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Print all column names\n",
    "print(\"CSV Columns:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Columns to drop\n",
    "columns_to_drop = [\"TYPE\", \"allow_walk\", \"allow_whee\", \"allow_cycl\", \n",
    "                   \"allow_pmd\", \"AMA_CATEGO\", \"Shape_Leng\", \"geometry_type\"]\n",
    "\n",
    "# Drop the specified columns\n",
    "df = df.drop(columns=columns_to_drop, errors=\"ignore\")  # Ignore errors if a column doesn't exist\n",
    "\n",
    "# Save the updated CSV file\n",
    "updated_csv_path = \"NParksTracks.csv\"\n",
    "df.to_csv(updated_csv_path, index=False)\n",
    "\n",
    "print(f\"✅ Updated CSV saved as: {updated_csv_path}\")\n",
    "\n",
    "print(\"CSV Columns:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast  # To safely evaluate string representation of lists\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = \"NParksTracks.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Step 1: Drop the 'FID' column\n",
    "df = df.drop(columns=['FID'], errors='ignore')\n",
    "\n",
    "# Step 2: Remove rows where 'PARK' is empty\n",
    "df = df[df['PARK'].notna() & df['PARK'].str.strip().ne('')]\n",
    "\n",
    "# Step 3: Combine rows with the same 'PARK'\n",
    "def combine_lists(series):\n",
    "    \"\"\"Convert to list, flattening if needed.\"\"\"\n",
    "    combined = []\n",
    "    for item in series.dropna():  # Drop NaN values\n",
    "        if isinstance(item, str):\n",
    "            try:\n",
    "                parsed_item = ast.literal_eval(item)  # Convert string to list\n",
    "                if isinstance(parsed_item, list):\n",
    "                    combined.extend(parsed_item)\n",
    "                else:\n",
    "                    combined.append(parsed_item)\n",
    "            except (SyntaxError, ValueError):\n",
    "                combined.append(item)  # Keep as string if conversion fails\n",
    "        else:\n",
    "            combined.append(item)\n",
    "    return list(set(combined))  # Remove duplicates\n",
    "\n",
    "def combine_coordinates(series):\n",
    "    \"\"\"Flatten and combine all 2D arrays into a single 2D array.\"\"\"\n",
    "    combined = []\n",
    "    for item in series.dropna():  # Drop NaN values\n",
    "        if isinstance(item, str):\n",
    "            try:\n",
    "                coords = ast.literal_eval(item)  # Convert string to list\n",
    "                if isinstance(coords, list):\n",
    "                    combined.extend(coords)  # Flatten into 2D array\n",
    "            except (SyntaxError, ValueError):\n",
    "                pass  # Skip invalid coordinates\n",
    "    return combined\n",
    "\n",
    "# Group by 'PARK' and apply the transformations\n",
    "df_grouped = df.groupby('PARK', as_index=False).agg({\n",
    "    'PARK_TYPE': lambda x: combine_lists(x),\n",
    "    'PCN_LOOP': lambda x: combine_lists(x),\n",
    "    'coordinates': lambda x: combine_coordinates(x)\n",
    "})\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "cleaned_csv_path = \"NParksTracks.csv\"\n",
    "df_grouped.to_csv(cleaned_csv_path, index=False)\n",
    "\n",
    "print(f\"✅ Cleaned CSV saved as: {cleaned_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning for PreSchools**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned CSV file saved: PreSchoolsLocation.csv\n",
      "Columns in the dataset:\n",
      "['Latitude', 'Longitude', 'CENTRE_NAME']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "excel_file_path = \"UncleanedData/PreSchoolsLocation_Full.xlsx\"  # Input Excel file\n",
    "csv_output_path = \"PreSchoolsLocation.csv\"  # Output CSV file\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Columns to drop\n",
    "columns_to_drop = [\"Name\", \"INC_CRC\", \"FMEL_UPD_D\", \"CENTRE_CODE\"]\n",
    "\n",
    "# Drop the specified columns\n",
    "df = df.drop(columns=columns_to_drop, errors=\"ignore\")  # Ignore if column not found\n",
    "\n",
    "# Save the cleaned DataFrame to CSV\n",
    "df.to_csv(csv_output_path, index=False)\n",
    "\n",
    "print(f\"✅ Cleaned CSV file saved: {csv_output_path}\")\n",
    "\n",
    "print(\"Columns in the dataset:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning for Primary / Secondary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned CSV file saved: Generalinformationofschools.csv\n",
      "Columns in the dataset:\n",
      "['school_name', 'address', 'postal_code', 'dgp_code', 'zone_code', 'mainlevel_code']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "csv_file_path = \"UncleanedData/Generalinformationofschools.csv\"  # Input CSV file\n",
    "csv_output_path = \"Generalinformationofschools.csv\"  # Output CSV file\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Print all column names, each on a new line\n",
    "# print(\"Columns in the dataset:\")\n",
    "# for col in df.columns:\n",
    "#     print(f\"- {col}\")  # Print each column separately\n",
    "\n",
    "# Columns to keep\n",
    "columns_to_keep = [\"school_name\", \"address\", \"postal_code\", \"dgp_code\", \"zone_code\", \"mainlevel_code\"]\n",
    "\n",
    "# Keep only the specified columns\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df.to_csv(csv_output_path, index=False)\n",
    "\n",
    "print(f\"✅ Cleaned CSV file saved: {csv_output_path}\")\n",
    "\n",
    "print(\"Columns in the dataset:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Updated CSV saved: Schools_with_Geolocation.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from Geocoding.geocoding import get_coordinates\n",
    "\n",
    "# Load your dataset\n",
    "csv_file_path = \"Generalinformationofschools.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Apply function to get coordinates\n",
    "df[\"Longitude\"], df[\"Latitude\"] = zip(*df[\"address\"].apply(get_coordinates))\n",
    "\n",
    "# Save the updated CSV\n",
    "updated_csv_path = \"ALLSCHOOLS.csv\"\n",
    "df.to_csv(updated_csv_path, index=False)\n",
    "\n",
    "print(f\"✅ Updated CSV saved: {updated_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'mainlevel_code':\n",
      "- PRIMARY\n",
      "- SECONDARY\n",
      "- JUNIOR COLLEGE\n",
      "- MIXED LEVELS\n",
      "- CENTRALISED INSTITUTE\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file path\n",
    "csv_file_path = \"ALLSCHOOLS.csv\"  # Update with the correct file path\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Get unique values in 'mainlevel_code' column\n",
    "unique_values = df[\"mainlevel_code\"].dropna().unique()  # Drop NaN values if any\n",
    "\n",
    "# Print unique values (one per line)\n",
    "print(\"Unique values in 'mainlevel_code':\")\n",
    "for value in unique_values:\n",
    "    print(f\"- {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned CSV file saved: PrimarySecondaryJC.csv\n",
      "Columns in the dataset:\n",
      "['school_name', 'mainlevel_code', 'Longitude', 'Latitude']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "csv_file_path = \"ALLSCHOOLS.csv\"  # Input CSV file\n",
    "csv_output_path = \"ALLSCHOOLS.csv\"  # Output CSV file\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Print all column names, each on a new line\n",
    "# print(\"Columns in the dataset:\")\n",
    "# for col in df.columns:\n",
    "#     print(f\"- {col}\")  # Print each column separately\n",
    "\n",
    "# Columns to keep\n",
    "columns_to_keep = [\"school_name\", \"mainlevel_code\", \"Longitude\",\"Latitude\"]\n",
    "\n",
    "# Keep only the specified columns\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df.to_csv(csv_output_path, index=False)\n",
    "\n",
    "print(f\"✅ Cleaned CSV file saved: {csv_output_path}\")\n",
    "\n",
    "print(\"Columns in the dataset:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: PRIMARY.csv\n",
      "✅ Saved: SECONDARY.csv\n",
      "✅ Saved: JUNIOR_COLLEGE.csv\n",
      "✅ Saved: MIXED_LEVELS.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file path\n",
    "csv_file_path = \"ALLSCHOOLS.csv\"  # Update with correct file path\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Define categories to filter\n",
    "categories = [\"PRIMARY\", \"SECONDARY\", \"JUNIOR COLLEGE\", \"MIXED LEVELS\"]\n",
    "\n",
    "# Save separate CSV files for each category\n",
    "for category in categories:\n",
    "    df_filtered = df[df[\"mainlevel_code\"] == category].drop(columns=[\"mainlevel_code\"], errors=\"ignore\")\n",
    "    output_path = f\"{category.replace(' ', '_')}.csv\"  # Replace spaces with underscores for filenames\n",
    "    df_filtered.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Saved: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined CSV file saved as: ALLSCHOOLS.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "primary_secondary_file = \"ALLSCHOOLS.csv\"  # Update with correct file path\n",
    "preschool_file = \"PreSchoolsLocation.csv\"  # Update with correct file path\n",
    "combined_output_file = \"ALLSCHOOLS.csv\"  # Output file\n",
    "\n",
    "# Load the primary/secondary/junior college dataset\n",
    "df_main = pd.read_csv(primary_secondary_file)\n",
    "\n",
    "# Load the preschool dataset\n",
    "df_preschool = pd.read_csv(preschool_file)\n",
    "\n",
    "# Ensure column names match for merging\n",
    "expected_columns = df_main.columns.tolist()\n",
    "\n",
    "# Create a new dataframe for preschools with matching columns\n",
    "df_preschool_mapped = pd.DataFrame(columns=expected_columns)\n",
    "\n",
    "# Map preschool data to the main dataset format\n",
    "df_preschool_mapped[\"school_name\"] = df_preschool[\"CENTRE_NAME\"]\n",
    "df_preschool_mapped[\"Latitude\"] = df_preschool[\"Latitude\"]\n",
    "df_preschool_mapped[\"Longitude\"] = df_preschool[\"Longitude\"]\n",
    "df_preschool_mapped[\"mainlevel_code\"] = \"PRESCHOOL\"  # Fill mainlevel_code with \"PRESCHOOL\"\n",
    "\n",
    "# Concatenate both datasets\n",
    "df_combined = pd.concat([df_main, df_preschool_mapped], ignore_index=True)\n",
    "\n",
    "# Save the combined dataset\n",
    "df_combined.to_csv(combined_output_file, index=False)\n",
    "\n",
    "print(f\"✅ Combined CSV file saved as: {combined_output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning for HDB Resale**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
