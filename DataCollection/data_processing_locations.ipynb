{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to check integrity of data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK IF COORDINATES ARE STORED IN A 2D ARRAY\n",
    "\n",
    "import pandas as pd\n",
    "import ast  # To safely evaluate string representation of lists\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = \"Data_Coordinates/LTAMRTStation.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "def is_valid_2d_array(coord_str):\n",
    "    \"\"\"\n",
    "    Check if a given string representation of coordinates is a valid 2D array.\n",
    "    - Must be a list of lists.\n",
    "    - Each inner list must contain exactly 2 elements (longitude, latitude).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        coordinates = ast.literal_eval(coord_str)  # Convert string back to list\n",
    "        if isinstance(coordinates, list) and all(\n",
    "            isinstance(coord, list) and len(coord) == 2 for coord in coordinates\n",
    "        ):\n",
    "            return True\n",
    "        return False\n",
    "    except (SyntaxError, ValueError):\n",
    "        return False\n",
    "\n",
    "# Apply the function to check each row in the \"coordinates\" column\n",
    "df[\"is_2D_valid\"] = df[\"coordinates\"].apply(is_valid_2d_array)\n",
    "\n",
    "# Print the rows where coordinates are NOT valid 2D arrays\n",
    "invalid_rows = df[~df[\"is_2D_valid\"]]\n",
    "\n",
    "# Display results\n",
    "if invalid_rows.empty:\n",
    "    print(\"✅ All values in the 'coordinates' column are valid 2D arrays.\")\n",
    "else:\n",
    "    print(f\"❌ Found {len(invalid_rows)} rows with invalid coordinates.\")\n",
    "    print(\"Here are the invalid rows:\")\n",
    "    print(invalid_rows[[\"geometry_type\", \"coordinates\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month                  0\n",
      "town                   0\n",
      "flat_type              0\n",
      "block                  0\n",
      "street_name            0\n",
      "storey_range           0\n",
      "floor_area_sqm         0\n",
      "flat_model             0\n",
      "lease_commence_date    0\n",
      "remaining_lease        0\n",
      "resale_price           0\n",
      "Latitude               0\n",
      "Longitude              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# CHECK IF NULL\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('Data_Coordinates/HDBResale_with_coordinates.csv')\n",
    "pd.set_option(\"display.max_rows\", None)  # Show all rows\n",
    "\n",
    "print(df.isnull().sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SportCenters ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "csv_file_path = \"Data_Raw/SportSGSportFacilities_Full.xlsx\"  # Input CSV file\n",
    "csv_output_path = \"Data_Coordinates/Sports.csv\"  # Output CSV file\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_excel(csv_file_path)\n",
    "\n",
    "# Print all column names, each on a new line\n",
    "print(\"Columns in the dataset:\")\n",
    "for col in df.columns:\n",
    "    print(f\"- {col}\")  # Print each column separately\n",
    "\n",
    "# Columns to keep\n",
    "columns_to_keep = [\"SPORTS_CEN\", \"Latitude\", \"Longitude\"]\n",
    "\n",
    "# Keep only the specified columns\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df.to_csv(csv_output_path, index=False)\n",
    "\n",
    "print(f\"✅ Cleaned CSV file saved: {csv_output_path}\")\n",
    "\n",
    "print(\"Columns in the dataset:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LTAMRTStations ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "excel_file_path = \"Data_Raw/LTAMRTStationExit_Full.xlsx\"  # Input Excel file\n",
    "csv_output_path = \"LTAMRTStation.csv\"  # Output CSV file\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Columns to drop\n",
    "columns_to_drop = [\"Name\", \"EXIT_CODE\", \"INC_CRC\", \"FMEL_UPD_D\"]\n",
    "\n",
    "# Drop the specified columns\n",
    "df = df.drop(columns=columns_to_drop, errors=\"ignore\")  # Ignore if column not found\n",
    "\n",
    "# Save the cleaned DataFrame to CSV\n",
    "df.to_csv(csv_output_path, index=False)\n",
    "\n",
    "print(f\"✅ Cleaned CSV file saved: {csv_output_path}\")\n",
    "\n",
    "print(\"Columns in the dataset:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "csv_file_path = \"LTAMRTStation.csv\"  # Input CSV file\n",
    "updated_csv_path = \"LTAMRTStation.csv\"  # Output CSV file\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Ensure 'Longitude' and 'Latitude' columns exist\n",
    "if \"Longitude\" in df.columns and \"Latitude\" in df.columns:\n",
    "    # Create a new 'coordinates' column with [[Longitude, Latitude]] format (2D array)\n",
    "    df[\"coordinates\"] = df.apply(lambda row: [[row[\"Longitude\"], row[\"Latitude\"]]], axis=1)\n",
    "\n",
    "    # Drop the original longitude and latitude columns\n",
    "    df = df.drop(columns=[\"Longitude\", \"Latitude\"], errors=\"ignore\")\n",
    "\n",
    "    # Save the updated DataFrame to the same CSV file\n",
    "    df.to_csv(updated_csv_path, index=False)\n",
    "\n",
    "    print(f\"✅ Updated CSV saved as: {updated_csv_path}\")\n",
    "else:\n",
    "    print(\"❌ 'Longitude' or 'Latitude' columns not found in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast  # To safely convert string representations of lists\n",
    "\n",
    "# Define file paths\n",
    "csv_file_path = \"LTAMRTStation.csv\"  # Input CSV file\n",
    "updated_csv_path = \"LTAMRTStation.csv\"  # Output CSV file\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Ensure required columns exist\n",
    "if \"STATION_NA\" in df.columns and \"coordinates\" in df.columns:\n",
    "    # Convert 'coordinates' column from string to actual lists\n",
    "    df[\"coordinates\"] = df[\"coordinates\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "    # Group by 'STATION_NA' and merge coordinates into a single 2D array per station\n",
    "    df_grouped = df.groupby(\"STATION_NA\", as_index=False).agg({\n",
    "        \"coordinates\": lambda x: [coord for sublist in x for coord in sublist]  # Flatten to single 2D array\n",
    "    })\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    df_grouped.to_csv(updated_csv_path, index=False)\n",
    "\n",
    "    print(f\"✅ Updated CSV saved as: {updated_csv_path}\")\n",
    "else:\n",
    "    print(\"❌ Required columns ('STATION_NA', 'coordinates') not found in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NPARKS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path_geojson = \"UncleanedData/NParksTracks.geojson\"  # Input GeoJSON file\n",
    "csv_output_path = \"NParksTracks.csv\"  # Output CSV file\n",
    "\n",
    "# Load the GeoJSON file\n",
    "with open(file_path_geojson, \"r\", encoding=\"utf-8\") as file:\n",
    "    geojson_data = json.load(file)\n",
    "\n",
    "# Extract relevant data from the GeoJSON\n",
    "features = geojson_data.get(\"features\", [])\n",
    "\n",
    "geojson_data_list = []\n",
    "\n",
    "def flatten_multiline_coordinates(multi_coords):\n",
    "    \"\"\"Flatten a 3D MultiLineString coordinates array into 2D.\"\"\"\n",
    "    return [coord for line in multi_coords for coord in line]\n",
    "\n",
    "for feature in features:\n",
    "    properties = feature.get(\"properties\", {})  # Extract properties\n",
    "    geometry = feature.get(\"geometry\", {})  # Extract geometry\n",
    "    coordinates = geometry.get(\"coordinates\", [])\n",
    "    geom_type = geometry.get(\"type\")\n",
    "\n",
    "    # Process coordinates based on geometry type\n",
    "    if geom_type == \"MultiLineString\":\n",
    "        coordinates = flatten_multiline_coordinates(coordinates)  # Flatten 3D to 2D\n",
    "    elif geom_type == \"LineString\":\n",
    "        pass  # Already 2D\n",
    "\n",
    "    # Store the processed data\n",
    "    properties[\"geometry_type\"] = geom_type\n",
    "    properties[\"coordinates\"] = json.dumps(coordinates)  # Store as JSON string for CSV compatibility\n",
    "    geojson_data_list.append(properties)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_geojson = pd.DataFrame(geojson_data_list)\n",
    "\n",
    "# Save the cleaned data to a CSV file\n",
    "df_geojson.to_csv(csv_output_path, index=False)\n",
    "\n",
    "print(f\"CSV file saved: {csv_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = \"NParksTracks.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Print all column names\n",
    "print(\"CSV Columns:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Columns to drop\n",
    "columns_to_drop = [\"TYPE\", \"allow_walk\", \"allow_whee\", \"allow_cycl\", \n",
    "                   \"allow_pmd\", \"AMA_CATEGO\", \"Shape_Leng\", \"geometry_type\"]\n",
    "\n",
    "# Drop the specified columns\n",
    "df = df.drop(columns=columns_to_drop, errors=\"ignore\")  # Ignore errors if a column doesn't exist\n",
    "\n",
    "# Save the updated CSV file\n",
    "updated_csv_path = \"NParksTracks.csv\"\n",
    "df.to_csv(updated_csv_path, index=False)\n",
    "\n",
    "print(f\"✅ Updated CSV saved as: {updated_csv_path}\")\n",
    "\n",
    "print(\"CSV Columns:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast  # To safely evaluate string representation of lists\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = \"NParksTracks.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Step 1: Drop the 'FID' column\n",
    "df = df.drop(columns=['FID'], errors='ignore')\n",
    "\n",
    "# Step 2: Remove rows where 'PARK' is empty\n",
    "df = df[df['PARK'].notna() & df['PARK'].str.strip().ne('')]\n",
    "\n",
    "# Step 3: Combine rows with the same 'PARK'\n",
    "def combine_lists(series):\n",
    "    \"\"\"Convert to list, flattening if needed.\"\"\"\n",
    "    combined = []\n",
    "    for item in series.dropna():  # Drop NaN values\n",
    "        if isinstance(item, str):\n",
    "            try:\n",
    "                parsed_item = ast.literal_eval(item)  # Convert string to list\n",
    "                if isinstance(parsed_item, list):\n",
    "                    combined.extend(parsed_item)\n",
    "                else:\n",
    "                    combined.append(parsed_item)\n",
    "            except (SyntaxError, ValueError):\n",
    "                combined.append(item)  # Keep as string if conversion fails\n",
    "        else:\n",
    "            combined.append(item)\n",
    "    return list(set(combined))  # Remove duplicates\n",
    "\n",
    "def combine_coordinates(series):\n",
    "    \"\"\"Flatten and combine all 2D arrays into a single 2D array.\"\"\"\n",
    "    combined = []\n",
    "    for item in series.dropna():  # Drop NaN values\n",
    "        if isinstance(item, str):\n",
    "            try:\n",
    "                coords = ast.literal_eval(item)  # Convert string to list\n",
    "                if isinstance(coords, list):\n",
    "                    combined.extend(coords)  # Flatten into 2D array\n",
    "            except (SyntaxError, ValueError):\n",
    "                pass  # Skip invalid coordinates\n",
    "    return combined\n",
    "\n",
    "# Group by 'PARK' and apply the transformations\n",
    "df_grouped = df.groupby('PARK', as_index=False).agg({\n",
    "    'PARK_TYPE': lambda x: combine_lists(x),\n",
    "    'PCN_LOOP': lambda x: combine_lists(x),\n",
    "    'coordinates': lambda x: combine_coordinates(x)\n",
    "})\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "cleaned_csv_path = \"NParksTracks.csv\"\n",
    "df_grouped.to_csv(cleaned_csv_path, index=False)\n",
    "\n",
    "print(f\"✅ Cleaned CSV saved as: {cleaned_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preschool ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "excel_file_path = \"UncleanedData/PreSchoolsLocation_Full.xlsx\"  # Input Excel file\n",
    "csv_output_path = \"PreSchoolsLocation.csv\"  # Output CSV file\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Columns to drop\n",
    "columns_to_drop = [\"Name\", \"INC_CRC\", \"FMEL_UPD_D\", \"CENTRE_CODE\"]\n",
    "\n",
    "# Drop the specified columns\n",
    "df = df.drop(columns=columns_to_drop, errors=\"ignore\")  # Ignore if column not found\n",
    "\n",
    "# Save the cleaned DataFrame to CSV\n",
    "df.to_csv(csv_output_path, index=False)\n",
    "\n",
    "print(f\"✅ Cleaned CSV file saved: {csv_output_path}\")\n",
    "\n",
    "print(\"Columns in the dataset:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary / Secondary / JC / Mixed ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "csv_file_path = \"UncleanedData/Generalinformationofschools.csv\"  # Input CSV file\n",
    "csv_output_path = \"Generalinformationofschools.csv\"  # Output CSV file\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Print all column names, each on a new line\n",
    "# print(\"Columns in the dataset:\")\n",
    "# for col in df.columns:\n",
    "#     print(f\"- {col}\")  # Print each column separately\n",
    "\n",
    "# Columns to keep\n",
    "columns_to_keep = [\"school_name\", \"address\", \"postal_code\", \"dgp_code\", \"zone_code\", \"mainlevel_code\"]\n",
    "\n",
    "# Keep only the specified columns\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df.to_csv(csv_output_path, index=False)\n",
    "\n",
    "print(f\"✅ Cleaned CSV file saved: {csv_output_path}\")\n",
    "\n",
    "print(\"Columns in the dataset:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Geocoding.geocoding import get_coordinates\n",
    "\n",
    "# Load your dataset\n",
    "csv_file_path = \"Generalinformationofschools.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Apply function to get coordinates\n",
    "df[\"Longitude\"], df[\"Latitude\"] = zip(*df[\"address\"].apply(get_coordinates))\n",
    "\n",
    "# Save the updated CSV\n",
    "updated_csv_path = \"ALLSCHOOLS.csv\"\n",
    "df.to_csv(updated_csv_path, index=False)\n",
    "\n",
    "print(f\"✅ Updated CSV saved: {updated_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file path\n",
    "csv_file_path = \"ALLSCHOOLS.csv\"  # Update with the correct file path\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Get unique values in 'mainlevel_code' column\n",
    "unique_values = df[\"mainlevel_code\"].dropna().unique()  # Drop NaN values if any\n",
    "\n",
    "# Print unique values (one per line)\n",
    "print(\"Unique values in 'mainlevel_code':\")\n",
    "for value in unique_values:\n",
    "    print(f\"- {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "csv_file_path = \"ALLSCHOOLS.csv\"  # Input CSV file\n",
    "csv_output_path = \"ALLSCHOOLS.csv\"  # Output CSV file\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Print all column names, each on a new line\n",
    "# print(\"Columns in the dataset:\")\n",
    "# for col in df.columns:\n",
    "#     print(f\"- {col}\")  # Print each column separately\n",
    "\n",
    "# Columns to keep\n",
    "columns_to_keep = [\"school_name\", \"mainlevel_code\", \"Longitude\",\"Latitude\"]\n",
    "\n",
    "# Keep only the specified columns\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df.to_csv(csv_output_path, index=False)\n",
    "\n",
    "print(f\"✅ Cleaned CSV file saved: {csv_output_path}\")\n",
    "\n",
    "print(\"Columns in the dataset:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file path\n",
    "csv_file_path = \"ALLSCHOOLS.csv\"  # Update with correct file path\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Define categories to filter\n",
    "categories = [\"PRIMARY\", \"SECONDARY\", \"JUNIOR COLLEGE\", \"MIXED LEVELS\"]\n",
    "\n",
    "# Save separate CSV files for each category\n",
    "for category in categories:\n",
    "    df_filtered = df[df[\"mainlevel_code\"] == category].drop(columns=[\"mainlevel_code\"], errors=\"ignore\")\n",
    "    output_path = f\"{category.replace(' ', '_')}.csv\"  # Replace spaces with underscores for filenames\n",
    "    df_filtered.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Saved: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "primary_secondary_file = \"ALLSCHOOLS.csv\"  # Update with correct file path\n",
    "preschool_file = \"PreSchoolsLocation.csv\"  # Update with correct file path\n",
    "combined_output_file = \"ALLSCHOOLS.csv\"  # Output file\n",
    "\n",
    "# Load the primary/secondary/junior college dataset\n",
    "df_main = pd.read_csv(primary_secondary_file)\n",
    "\n",
    "# Load the preschool dataset\n",
    "df_preschool = pd.read_csv(preschool_file)\n",
    "\n",
    "# Ensure column names match for merging\n",
    "expected_columns = df_main.columns.tolist()\n",
    "\n",
    "# Create a new dataframe for preschools with matching columns\n",
    "df_preschool_mapped = pd.DataFrame(columns=expected_columns)\n",
    "\n",
    "# Map preschool data to the main dataset format\n",
    "df_preschool_mapped[\"school_name\"] = df_preschool[\"CENTRE_NAME\"]\n",
    "df_preschool_mapped[\"Latitude\"] = df_preschool[\"Latitude\"]\n",
    "df_preschool_mapped[\"Longitude\"] = df_preschool[\"Longitude\"]\n",
    "df_preschool_mapped[\"mainlevel_code\"] = \"PRESCHOOL\"  # Fill mainlevel_code with \"PRESCHOOL\"\n",
    "\n",
    "# Concatenate both datasets\n",
    "df_combined = pd.concat([df_main, df_preschool_mapped], ignore_index=True)\n",
    "\n",
    "# Save the combined dataset\n",
    "df_combined.to_csv(combined_output_file, index=False)\n",
    "\n",
    "print(f\"✅ Combined CSV file saved as: {combined_output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
